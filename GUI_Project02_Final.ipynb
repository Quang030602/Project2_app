{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GmdRRaDuK7TG","outputId":"44b31ab2-f38a-4656-dfa9-c609e2e1db80","executionInfo":{"status":"ok","timestamp":1751111587622,"user_tz":-420,"elapsed":9662,"user":{"displayName":"Thao Nguyen","userId":"13621871980693989521"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.46.1)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n","Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.2.1)\n","Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.26.4)\n","Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n","Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.5)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.5.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.14.0)\n","Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n","Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n","Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.24.0)\n","Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.44.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.6.15)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n","Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.25.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n","Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.11)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n"]}],"source":["!pip3 install streamlit\n","!pip3 install pyngrok"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"d-B49j6yLLKn","executionInfo":{"status":"ok","timestamp":1751111596636,"user_tz":-420,"elapsed":9004,"user":{"displayName":"Thao Nguyen","userId":"13621871980693989521"}}},"outputs":[],"source":["# Step 1: Install required packages\n","!pip install streamlit -q\n","!pip install pyngrok -q"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4159,"status":"ok","timestamp":1751111600800,"user":{"displayName":"Thao Nguyen","userId":"13621871980693989521"},"user_tz":-420},"id":"N-kTObYM2JUN","outputId":"86f3b84c-51c9-4557-e722-e0e754282cc8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/gdrive\", force_remount=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":53,"status":"ok","timestamp":1751111600858,"user":{"displayName":"Thao Nguyen","userId":"13621871980693989521"},"user_tz":-420},"id":"MxQ1aa_n2P35","outputId":"111d08e9-1e20-4f4a-991d-d2dac36d898c"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/DL07_DATN_k304_T37_NguyenQuynhOanhThao_LeNguyenMinhQuang/Project 02/Du lieu cung cap\n"]}],"source":["%cd '/content/gdrive/MyDrive/DL07_DATN_k304_T37_NguyenQuynhOanhThao_LeNguyenMinhQuang/Project 02/Du lieu cung cap'"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3823,"status":"ok","timestamp":1751111604678,"user":{"displayName":"Thao Nguyen","userId":"13621871980693989521"},"user_tz":-420},"id":"ENNWg9F-qUgo","outputId":"6e856f43-3576-47de-e90f-71f590f47b37"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n","Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n","Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"]}],"source":["!pip install gensim"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5738,"status":"ok","timestamp":1751111610417,"user":{"displayName":"Thao Nguyen","userId":"13621871980693989521"},"user_tz":-420},"id":"5C9wWm8WTmsn","outputId":"ff9197ba-fc94-4cee-94a8-907183c3db50"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":6}],"source":["    import os\n","    os.system('pip install underthesea')"]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46,"status":"ok","timestamp":1751120144693,"user":{"displayName":"Thao Nguyen","userId":"13621871980693989521"},"user_tz":-420},"id":"n_2OkWnvLdG0","outputId":"cc4ddbbb-7125-4efb-996e-9ee3682d1fb2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}],"source":["# Step 2: Write the Streamlit app to a Python file\n","%%writefile app.py\n","import streamlit as st\n","\n","st.title(\"Project 02 - Company Recommendation & Candidate Classification\")\n","st.caption(\"Team: Nguyen Quynh Oanh Thao - Nguyen Le Minh Quang\")\n","\n","menu = [\"Home\", \"About\"]\n","choice = st.sidebar.selectbox('Menu', menu)\n","\n","\n","if choice == 'Home':\n","   st.subheader(\"Streamlit From Colab\")\n","elif choice == 'About':\n","   st.subheader(\"Requirements info\")\n","\n","st.set_page_config(page_title=\"Project 02\", layout=\"wide\")\n","\n","# Tabs for Topic 1 and Topic 2\n","tab1, tab2 = st.tabs([\"üîç Topic 1: Company Recommendation\", \"üß† Topic 2: Candidate Classification\"])\n","\n","with tab1:\n","    st.header(\"Topic 1: Content-Based Company Recommendation System\")\n","\n","\n","with tab1:\n","    import pandas as pd\n","    import numpy as np\n","    import streamlit as st\n","    import re\n","    import gensim\n","    from sklearn.feature_extraction.text import TfidfVectorizer\n","    from sklearn.model_selection import train_test_split\n","    from sklearn.preprocessing import LabelEncoder\n","    from sklearn.linear_model import LogisticRegression\n","    from sklearn.metrics.pairwise import cosine_similarity\n","    from gensim import models as gensim_models, corpora, similarities\n","    from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix\n","    import matplotlib.pyplot as plt\n","    import seaborn as sns\n","    from scipy.sparse import hstack\n","    from sklearn.neighbors import KNeighborsClassifier\n","    from sklearn.tree import DecisionTreeClassifier\n","    from sklearn.ensemble import RandomForestClassifier\n","    from sklearn.svm import SVC\n","\n","    # --- Text Preprocessing ---\n","    def clean_tokens(tokens):\n","        cleaned = [re.sub(r'\\d+', '', word) for word in tokens]\n","        return [word.lower() for word in cleaned if word not in ['', ' ', ',', '.', '-', ':', '?', '%', '(', ')', '+', '/', 'g', 'ml']]\n","\n","    stop_words = set([\n","        \"a\", \"an\", \"the\", \"in\", \"on\", \"at\", \"to\", \"from\", \"by\", \"of\", \"with\", \"and\", \"but\", \"or\", \"for\", \"nor\", \"so\", \"yet\",\n","        \"i\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"him\", \"her\", \"us\", \"them\", \"be\", \"have\", \"do\", \"does\", \"did\",\n","        \"was\", \"were\", \"will\", \"would\", \"shall\", \"should\", \"may\", \"might\", \"can\", \"could\", \"must\",\n","        \"that\", \"this\", \"which\", \"what\", \"their\", \"these\", \"those\", \"https\", \"www\"\n","    ])\n","\n","    def remove_stopwords(tokens):\n","        return [word for word in tokens if word not in stop_words]\n","\n","    # --- Load and Prepare Data ---\n","    @st.cache_data\n","    def load_and_process_data():\n","        df = pd.read_excel(\"/content/gdrive/MyDrive/DL07_DATN_k304_T37_NguyenQuynhOanhThao_LeNguyenMinhQuang/Project 02/Du lieu cung cap/Overview_Companies.xlsx\")\n","        df = df[['Company Name', 'Company overview']].dropna().copy()\n","        df['tokens'] = df['Company overview'].apply(lambda x: gensim.utils.simple_preprocess(x))\n","        df['tokens_cleaned'] = df['tokens'].apply(clean_tokens)\n","        df['tokens_final'] = df['tokens_cleaned'].apply(remove_stopwords)\n","        df = df[df['tokens_final'].str.len() > 0].copy()\n","        df['joined_tokens'] = df['tokens_final'].apply(lambda tokens: ' '.join(tokens))\n","        return df\n","\n","    df = load_and_process_data()\n","\n","    # --- ML Classification ---\n","    vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 3), sublinear_tf=True, stop_words='english', min_df=2, max_df=0.8, norm='l2')\n","    X = vectorizer.fit_transform(df['joined_tokens'])\n","\n","    # Dummy label for now: use KMeans or known labels\n","    from sklearn.cluster import KMeans\n","    kmeans = KMeans(n_clusters=3, random_state=42)\n","    df['label'] = kmeans.fit_predict(X)\n","    label_map = {0: 'Low', 1: 'Medium', 2: 'High'}\n","    df['label'] = df['label'].map(label_map)\n","\n","    # Encode & split\n","    le = LabelEncoder()\n","    y = le.fit_transform(df['label'])\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n","\n","    clf = LogisticRegression(max_iter=1000, class_weight='balanced')\n","    clf.fit(X_train, y_train)\n","    y_pred = clf.predict(X_test)\n","\n","    # --- Gensim Similarity Setup ---\n","    dictionary = corpora.Dictionary(df['tokens_final'])\n","    corpus = [dictionary.doc2bow(text) for text in df['tokens_final']]\n","    tfidf_model = gensim_models.TfidfModel(corpus)\n","    corpus_tfidf = tfidf_model[corpus]\n","    index = similarities.SparseMatrixSimilarity(corpus_tfidf, num_features=len(dictionary))\n","\n","    # --- Charts -----\n","    # ---- Define models BEFORE using them ----\n","    models = {\n","    \"Logistic Regression\": LogisticRegression(max_iter=1000, class_weight='balanced'),\n","    \"K-Nearest Neighbors\": KNeighborsClassifier(n_neighbors=5),\n","    \"Decision Tree\": DecisionTreeClassifier(class_weight='balanced'),\n","    \"Random Forest\": RandomForestClassifier(n_estimators=200, class_weight='balanced'),\n","    \"Support Vector Machine\": SVC(probability=True, class_weight='balanced')}\n","    # ---- Recalculate cosine feature and split ----\n","    cosine_sim = cosine_similarity(X, X)\n","    ref_sim = cosine_sim[0].reshape(-1, 1)\n","    X_with_sim = hstack([X, ref_sim])\n","    X_train, X_test, y_train, y_test = train_test_split(\n","    X_with_sim, y, test_size=0.5, random_state=42, stratify=y)\n","\n","    # ---- Evaluate All Models ----\n","    results = {}\n","    for name, model in models.items():\n","      model.fit(X_train, y_train)\n","      y_pred = model.predict(X_test)\n","      precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n","      recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n","      f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n","      accuracy = accuracy_score(y_test, y_pred)\n","      results[name] = {\n","          \"Accuracy\": accuracy,\n","          \"Precision\": precision,\n","          \"Recall\": recall,\n","          \"F1-score\": f1\n","    }\n","\n","    # ---- Display Performance Table ----\n","    st.write(\"## üìä Model Performance Summary\")\n","    st.dataframe(pd.DataFrame(results).T.sort_values(by=\"F1-score\", ascending=False))\n","\n","    # ---- Confusion Matrix Visualization ----\n","    st.write(\"## üîç Confusion Matrices\")\n","    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n","    axes = axes.flatten()\n","\n","    for idx, (name, model) in enumerate(models.items()):\n","      y_pred = model.predict(X_test)\n","      cm = confusion_matrix(y_test, y_pred)\n","      sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])\n","      axes[idx].set_title(f\"{name}\")\n","      axes[idx].set_xlabel(\"Predicted\")\n","      axes[idx].set_ylabel(\"Actual\")\n","\n","    # Remove extra axes\n","    for j in range(len(models), len(axes)):\n","      fig.delaxes(axes[j])\n","\n","    st.pyplot(fig)\n","\n","    # --- Show selected model ---\n","    st.markdown(\"## ‚úÖ Selected Model for Recommendation\")\n","    st.write(\"\"\"We are using **Logistic Regression** as the primary model for predicting company fit and driving the similarity search logic below, based on its strong F1-score and overall performance.\n","\"\"\")\n","\n","\n","    # --- Streamlit UI ---\n","    st.subheader(\"üîç Explore 'High' Fit Companies (Gensim TF-IDF Similarity)\")\n","\n","    input_text = st.text_area(\"Enter your company description or summary:\")\n","    if st.button(\"Find similar 'High' fit companies\"):\n","        if not input_text.strip():\n","            st.warning(\"Please enter some text.\")\n","        else:\n","            input_tokens = gensim.utils.simple_preprocess(input_text)\n","            input_tokens_clean = remove_stopwords(clean_tokens(input_tokens))\n","            input_bow = dictionary.doc2bow(input_tokens_clean)\n","\n","            sims = index[tfidf_model[input_bow]]\n","            ranked = sorted(enumerate(sims), key=lambda x: -x[1])\n","\n","            st.write(\"### Top Similar Companies (label = High)\")\n","            count = 0\n","            for idx, score in ranked:\n","                if df.iloc[idx]['label'] == 'High':\n","                    st.markdown(f\"#### üè∑Ô∏è {df.iloc[idx]['Company Name']}\")\n","                    st.markdown(f\"- **Similarity Score:** `{score:.2f}`\")\n","                    st.markdown(f\"> {df.iloc[idx]['Company overview']}\")\n","                    st.markdown(\"---\")\n","                    count += 1\n","                if count >= 5:\n","                    break\n","\n","with tab2:\n","    st.header(\"Topic 2: Candidate Fit Classification\")\n","\n","\n","    import pandas as pd\n","    import numpy as np\n","    import streamlit as st\n","    import matplotlib.pyplot as plt\n","    import seaborn as sns\n","    import re\n","    import openpyxl\n","    from underthesea import word_tokenize\n","\n","    from sklearn.model_selection import train_test_split\n","    from sklearn.feature_extraction.text import TfidfVectorizer\n","    from sklearn.preprocessing import LabelEncoder, StandardScaler\n","    from sklearn.linear_model import LogisticRegression\n","    from sklearn.svm import SVC\n","    from sklearn.ensemble import RandomForestClassifier\n","    from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, roc_curve, roc_auc_score\n","    from imblearn.over_sampling import SMOTE\n","    from collections import Counter\n","\n","\n","    # --- Load data from Google Drive local paths ---\n","    @st.cache_data\n","    def load_review_data():\n","        reviews = pd.read_excel(\"/content/gdrive/MyDrive/DL07_DATN_k304_T37_NguyenQuynhOanhThao_LeNguyenMinhQuang/Project 02/Du lieu cung cap/Reviews.xlsx\")\n","        overview_reviews = pd.read_excel(\"/content/gdrive/MyDrive/DL07_DATN_k304_T37_NguyenQuynhOanhThao_LeNguyenMinhQuang/Project 02/Du lieu cung cap/Overview_Reviews.xlsx\")\n","        overview_companies = pd.read_excel(\"/content/gdrive/MyDrive/DL07_DATN_k304_T37_NguyenQuynhOanhThao_LeNguyenMinhQuang/Project 02/Du lieu cung cap/Overview_Companies.xlsx\")\n","\n","        overview_reviews = overview_reviews.rename(columns={\"id\": \"company_id\"})\n","        overview_companies = overview_companies.rename(columns={\"id\": \"company_id\"})\n","\n","        data = reviews.merge(overview_reviews[[\"company_id\", \"Overall rating\"]], left_on=\"id\", right_on=\"company_id\", how=\"left\")\n","        data = data.merge(overview_companies[[\"company_id\", \"Company Name\", \"Company Type\", \"Company size\"]], on=\"company_id\", how=\"left\")\n","\n","        st.write(\"üìé Available columns:\", data.columns.tolist())\n","        return data\n","\n","    # Load data\n","    df_reviews = load_review_data()\n","\n","    # Validate required columns\n","    if 'What I liked' not in df_reviews.columns or 'Suggestions for improvement' not in df_reviews.columns:\n","        st.error(\"‚ùå Required columns 'What I liked' or 'Suggestions for improvement' are missing in the dataset.\")\n","        st.stop()\n","\n","    # Load stopwords and wrong words\n","    with open(\"/content/gdrive/MyDrive/DL07_DATN_k304_T37_NguyenQuynhOanhThao_LeNguyenMinhQuang/Project 02/Du lieu cung cap/files/vietnamese-stopwords.txt\", encoding=\"utf-8\") as f:\n","        stopwords = set(f.read().splitlines())\n","\n","    with open(\"/content/gdrive/MyDrive/DL07_DATN_k304_T37_NguyenQuynhOanhThao_LeNguyenMinhQuang/Project 02/Du lieu cung cap/files/wrong-word.txt\", encoding=\"utf-8\") as f:\n","        wrong_words = set(f.read().splitlines())\n","\n","    # Clean text function\n","    def clean_text(text):\n","        if pd.isnull(text):\n","            return \"\"\n","        text = str(text).lower()\n","        text = re.sub(r'[^a-zA-Z0-9√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµ\\s]', ' ', text)\n","        text = re.sub(r'\\d+', ' ', text)\n","        text = re.sub(r'\\s+', ' ', text)\n","        text = word_tokenize(text, format=\"text\")\n","        words = [w for w in text.split() if w not in stopwords and w not in wrong_words and len(w) > 2]\n","        return \" \".join(words)\n","\n","    # Suggestion classification function\n","    def suggest_improvement(text):\n","        if pd.isnull(text):\n","            return \"Kh√¥ng c√≥ g√≥p √Ω ti√™u c·ª±c\"\n","        text = str(text).lower()\n","        negative_keywords = [\n","            \"kh√¥ng\", \"thi·∫øu\", \"ch∆∞a\", \"overtime\", \"l∆∞∆°ng_th·∫•p\",\n","            \"√°p_l·ª±c\", \"ch·∫≠m\", \"t·ªá\", \"b·∫•t_c√¥ng\", \"qu√°_t·∫£i\", \"stress\"\n","        ]\n","        for kw in negative_keywords:\n","            if kw in text:\n","                return \"C·∫ßn c·∫£i thi·ªán: \" + kw.replace(\"_\", \" \")\n","        return \"Kh√¥ng c√≥ g√≥p √Ω ti√™u c·ª±c\"\n","\n","    # Apply cleaning\n","    df_reviews['What I liked_clean'] = df_reviews['What I liked'].apply(clean_text)\n","    df_reviews['Suggestions_clean'] = df_reviews['Suggestions for improvement'].apply(suggest_improvement)\n","    df_reviews['text_combined'] = df_reviews['What I liked_clean'] + ' ' + df_reviews['Suggestions_clean']\n","\n","    # Feature engineering\n","    features = df_reviews[['text_combined', 'Rating', 'Company Type', 'Company size', 'Overall rating']]\n","    target = df_reviews['Recommend?']\n","\n","    # Encode categorical variables\n","    categorical_cols = ['Company Type', 'Company size']\n","    features_processed = pd.get_dummies(features, columns=categorical_cols, drop_first=True)\n","\n","\n","    # Chuy·ªÉn ƒë·ªïi text th√†nh TF-IDF features\n","    tfidf = TfidfVectorizer(max_features=1000, ngram_range=(1,2))\n","    text_features = tfidf.fit_transform(features['text_combined'])\n","    # K·∫øt h·ª£p text features v√† c√°c features s·ªë kh√°c\n","    # Drop 'text_combined' and convert to float (after ensuring all are numeric)\n","    numeric_features = features_processed.drop('text_combined', axis=1).apply(pd.to_numeric, errors='coerce').fillna(0).values.astype('float64')\n","    X = hstack((text_features, numeric_features))\n","    y = target\n","\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","    smote = SMOTE(random_state=42)\n","    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n","\n","\n","    #----------------\n","    # Only keeping Logistic Regression, SVM, and XGBoost\n","    # Removing Random Forest, Spark models, and related evaluation logic\n","    # The model training and evaluation will focus on these three models\n","\n","    import pandas as pd\n","    import matplotlib.pyplot as plt\n","    from sklearn.linear_model import LogisticRegression\n","    from sklearn.svm import SVC\n","    from xgboost import XGBClassifier\n","    from sklearn.metrics import (\n","        classification_report, roc_auc_score, roc_curve,\n","        accuracy_score, precision_score, recall_score, f1_score\n","    )\n","    from sklearn.preprocessing import LabelEncoder\n","\n","    # --- Model Training ---\n","\n","    lr_model = LogisticRegression(max_iter=1000, random_state=42)\n","    lr_model.fit(X_train_balanced, y_train_balanced)\n","    lr_pred = lr_model.predict(X_test)\n","    lr_proba = lr_model.predict_proba(X_test)[:, 1]\n","    lr_auc = roc_auc_score(y_test == \"Yes\", lr_proba)\n","    lr_fpr, lr_tpr, _ = roc_curve(y_test == \"Yes\", lr_proba)\n","\n","    svm_model = SVC(probability=True, random_state=42)\n","    svm_model.fit(X_train_balanced, y_train_balanced)\n","    svm_pred = svm_model.predict(X_test)\n","    svm_proba = svm_model.predict_proba(X_test)[:, 1]\n","    svm_auc = roc_auc_score(y_test == \"Yes\", svm_proba)\n","    svm_fpr, svm_tpr, _ = roc_curve(y_test == \"Yes\", svm_proba)\n","\n","    label_encoder = LabelEncoder()\n","    y_train_enc = label_encoder.fit_transform(y_train_balanced)\n","    y_test_enc = label_encoder.transform(y_test)\n","    xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n","    xgb_model.fit(X_train_balanced, y_train_enc)\n","    xgb_pred_enc = xgb_model.predict(X_test)\n","    xgb_proba = xgb_model.predict_proba(X_test)[:, 1]\n","    xgb_pred = label_encoder.inverse_transform(xgb_pred_enc)\n","    xgb_auc = roc_auc_score(y_test == \"Yes\", xgb_proba)\n","    xgb_fpr, xgb_tpr, _ = roc_curve(y_test == \"Yes\", xgb_proba)\n","\n","    # --- Visualization ---\n","\n","    plt.figure(figsize=(10, 8))\n","    plt.plot(lr_fpr, lr_tpr, label=f\"Logistic Regression (AUC = {lr_auc:.4f})\")\n","    plt.plot(svm_fpr, svm_tpr, label=f\"SVM (AUC = {svm_auc:.4f})\")\n","    plt.plot(xgb_fpr, xgb_tpr, label=f\"XGBoost (AUC = {xgb_auc:.4f})\")\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlabel(\"False Positive Rate\")\n","    plt.ylabel(\"True Positive Rate\")\n","    plt.title(\"ROC Curve - Selected Models\")\n","    plt.legend()\n","    plt.grid(True)\n","    plt.tight_layout()\n","    plt.show()\n","\n","    # --- Metrics Comparison ---\n","\n","    models_comparison = pd.DataFrame({\n","        \"Model\": [\"Logistic Regression\", \"SVM\", \"XGBoost\"],\n","        \"AUC\": [lr_auc, svm_auc, xgb_auc],\n","        \"Accuracy\": [\n","            accuracy_score(y_test, lr_pred),\n","            accuracy_score(y_test, svm_pred),\n","            accuracy_score(y_test, xgb_pred)\n","        ],\n","        \"Precision\": [\n","            precision_score(y_test, lr_pred, pos_label=\"Yes\"),\n","            precision_score(y_test, svm_pred, pos_label=\"Yes\"),\n","            precision_score(y_test, xgb_pred, pos_label=\"Yes\")\n","        ],\n","        \"Recall\": [\n","            recall_score(y_test, lr_pred, pos_label=\"Yes\"),\n","            recall_score(y_test, svm_pred, pos_label=\"Yes\"),\n","            recall_score(y_test, xgb_pred, pos_label=\"Yes\")\n","        ],\n","        \"F1-Score\": [\n","            f1_score(y_test, lr_pred, pos_label=\"Yes\"),\n","            f1_score(y_test, svm_pred, pos_label=\"Yes\"),\n","            f1_score(y_test, xgb_pred, pos_label=\"Yes\")\n","        ]\n","    })\n","\n","    print(\"\\nModel Performance Comparison:\")\n","    print(models_comparison.sort_values(\"AUC\", ascending=False))\n","\n","    # --- ROC Curve Display inside Streamlit ---\n","    st.subheader(\"üìâ ROC Curve - Logistic Regression vs SVM vs XGBoost\")\n","\n","    fig_roc, ax = plt.subplots(figsize=(10, 6))\n","    ax.plot(lr_fpr, lr_tpr, label=f\"Logistic Regression (AUC = {lr_auc:.4f})\")\n","    ax.plot(svm_fpr, svm_tpr, label=f\"SVM (AUC = {svm_auc:.4f})\")\n","    ax.plot(xgb_fpr, xgb_tpr, label=f\"XGBoost (AUC = {xgb_auc:.4f})\")\n","    ax.plot([0, 1], [0, 1], 'k--')\n","    ax.set_xlabel(\"False Positive Rate\")\n","    ax.set_ylabel(\"True Positive Rate\")\n","    ax.set_title(\"ROC Curve - Selected Models\")\n","    ax.legend()\n","    ax.grid(True)\n","    st.pyplot(fig_roc)\n","\n","    # --- Metrics Comparison Table ---\n","    st.subheader(\"üìä Model Performance Summary\")\n","    st.dataframe(models_comparison.sort_values(\"AUC\", ascending=False).round(4))\n","\n","    import streamlit as st\n","    import pandas as pd\n","    import numpy as np\n","    import joblib\n","\n","    from sklearn.preprocessing import OneHotEncoder\n","\n","    # --- App Settings ---\n","    st.set_page_config(page_title=\"ITViec Review Recommendation Predictor\")\n","    st.title(\"üîç Predict 'Recommend' from Employee Review\")\n","    st.markdown(\"D·ª±a tr√™n th√¥ng tin ƒë√°nh gi√° t·ª´ nh√¢n vi√™n ƒë√£ review tr√™n ITViec, d·ª± ƒëo√°n xem h·ªç c√≥ recommend c√¥ng ty hay kh√¥ng.\")\n","\n","    # --- Load Pre-trained Models and Encoders ---\n","    xgb_model = joblib.load(\"xgb_model.pkl\")\n","    label_encoder = joblib.load(\"label_encoder.pkl\")\n","    onehot_encoder = joblib.load(\"onehot_encoder.pkl\")\n","\n","    # --- Load Company Data ---\n","    overview_companies = pd.read_excel(\n","        \"/content/gdrive/MyDrive/DL07_DATN_k304_T37_NguyenQuynhOanhThao_LeNguyenMinhQuang/Project 02/Du lieu cung cap/Overview_Companies.xlsx\"\n","    )\n","    overview_companies = overview_companies.rename(columns={\"Company Name\": \"company_name\"})\n","\n","    # --- Prediction UI ---\n","    st.subheader(\"üìù D·ª± ƒëo√°n theo t√™n c√¥ng ty\")\n","\n","    company_name_list = overview_companies[\"company_name\"].dropna().unique().tolist()\n","    company_name = st.selectbox(\"Ch·ªçn t√™n c√¥ng ty\", sorted(company_name_list))\n","\n","    if st.button(\"D·ª± ƒëo√°n\"):\n","        try:\n","            # L·∫•y th√¥ng tin c√¥ng ty t·ª´ t√™n ƒë√£ ch·ªçn\n","            selected_info = overview_companies[overview_companies[\"company_name\"] == company_name].iloc[0]\n","            company_type = selected_info[\"Company Type\"]\n","            company_size = selected_info[\"Company size\"]\n","\n","            # Encode categorical features\n","            cat_features = pd.DataFrame([[company_type, company_size]], columns=[\"Company Type\", \"Company size\"])\n","            cat_encoded = onehot_encoder.transform(cat_features)  # Do not use .toarray() if sparse_output=False\n","\n","            # T·∫°o m·∫£ng 1010 chi·ªÅu, ch√®n cat_encoded v√†o cu·ªëi\n","            num_total_features = 1010  # s·ªë chi·ªÅu model y√™u c·∫ßu\n","            num_cat_features = cat_encoded.shape[1]\n","\n","            # Kh·ªüi t·∫°o m·∫£ng to√†n 0\n","            final_features = np.zeros((1, num_total_features))\n","\n","            # G√°n cat_encoded v√†o ph·∫ßn cu·ªëi\n","            final_features[0, -num_cat_features:] = cat_encoded\n","\n","            # D·ª± ƒëo√°n\n","            proba = xgb_model.predict_proba(final_features)[0][1]\n","            prediction = label_encoder.inverse_transform([int(proba >= 0.5)])[0]\n","\n","\n","\n","            # Output\n","            st.subheader(\"üîç K·∫øt qu·∫£\")\n","            st.write(f\"**X√°c su·∫•t Recommend:** {proba:.2%}\")\n","            st.success(f\"‚ú® D·ª± ƒëo√°n: **{prediction}**\")\n","\n","        except IndexError:\n","            st.error(\"‚ùå Kh√¥ng t√¨m th·∫•y th√¥ng tin c√¥ng ty. Vui l√≤ng th·ª≠ l·∫°i.\")\n","        except Exception as e:\n","            st.error(f\"‚ùå L·ªói: {str(e)}\")\n","\n","\n","\n","\n","\n","\n","    import streamlit as st"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"NoZ9oYZoLrRz","executionInfo":{"status":"ok","timestamp":1751116019156,"user_tz":-420,"elapsed":2,"user":{"displayName":"Thao Nguyen","userId":"13621871980693989521"}}},"outputs":[],"source":["# Step 3: Import ngrok\n","from pyngrok import ngrok\n"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"mpbAkPksLs0Q","executionInfo":{"status":"ok","timestamp":1751116020263,"user_tz":-420,"elapsed":25,"user":{"displayName":"Thao Nguyen","userId":"13621871980693989521"}}},"outputs":[],"source":["# Step 4: Set ngrok authentication token\n","ngrok.set_auth_token(\"2yzW7LSuACBzkvxTIUcS0XN6Yir_2MrZqfzZc78W7zeerDqHy\")  # ‚Üê Replace with your actual ngrok token"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":184,"status":"ok","timestamp":1751116021248,"user":{"displayName":"Thao Nguyen","userId":"13621871980693989521"},"user_tz":-420},"id":"cqWMopgJL1ii","outputId":"eb63bfa9-414b-4193-816e-49a35a2cbf5b"},"outputs":[{"output_type":"stream","name":"stdout","text":["nohup: appending output to 'nohup.out'\n","* Tunnel URL: https://552e-34-169-94-140.ngrok-free.app\n"]}],"source":["# Step 5: Run the Streamlit app and create a public tunnel\n","!nohup streamlit run app.py --server.port 8501 &\n","\n","# Open a tunnel to the streamlit port\n","ngrok_tunnel = ngrok.connect(addr='8501', proto='http')\n","\n","# Print the public URL\n","print('* Tunnel URL:', ngrok_tunnel.public_url)"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"fhYLohi1MrQd","executionInfo":{"status":"ok","timestamp":1751111612924,"user_tz":-420,"elapsed":1,"user":{"displayName":"Thao Nguyen","userId":"13621871980693989521"}}},"outputs":[],"source":["# Step 6: Stop the app (run only when needed)\n","#ngrok.kill()"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":1,"status":"ok","timestamp":1751111612927,"user":{"displayName":"Thao Nguyen","userId":"13621871980693989521"},"user_tz":-420},"id":"czX6FbQdHSge"},"outputs":[],"source":["# Save your notebook as .py\n","#!jupyter nbconvert --to script /content/your_notebook_name.ipynb"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM3/OUbYzcFDP78lGnfan66"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}