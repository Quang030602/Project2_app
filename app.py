import streamlit as st
import pandas as pd
import numpy as np
import re
import os
import warnings

# Core ML imports with error handling
try:
    import joblib
    JOBLIB_AVAILABLE = True
except ImportError:
    JOBLIB_AVAILABLE = False
    st.warning("‚ö†Ô∏è joblib not available. Model loading will be disabled.")

try:
    import matplotlib.pyplot as plt
    import seaborn as sns
    PLOTTING_AVAILABLE = True
except ImportError:
    PLOTTING_AVAILABLE = False
    st.warning("‚ö†Ô∏è matplotlib/seaborn not available. Plots will be disabled.")

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import LabelEncoder, StandardScaler
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics.pairwise import cosine_similarity
    from sklearn.cluster import KMeans
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.tree import DecisionTreeClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.svm import SVC
    from sklearn.metrics import (
        precision_score, recall_score, f1_score, accuracy_score, confusion_matrix,
        roc_auc_score, roc_curve
    )
    SKLEARN_AVAILABLE = True
except ImportError:
    SKLEARN_AVAILABLE = False
    st.error("‚ùå scikit-learn not available. ML features will be disabled.")

try:
    import gensim
    from gensim import models as gensim_models, corpora, similarities
    GENSIM_AVAILABLE = True
except ImportError:
    GENSIM_AVAILABLE = False
    # Create dummy objects to prevent NameError
    gensim = None
    gensim_models = None
    corpora = None
    similarities = None

try:
    from scipy.sparse import hstack
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False

try:
    import xgboost as xgb
    XGBOOST_AVAILABLE = True
except ImportError:
    XGBOOST_AVAILABLE = False

# PAGE CONFIG (n√™n ƒë·ªÉ ƒë·∫ßu file)
st.set_page_config(page_title="Project 02 - Company Recommendation & Candidate Classification", layout="wide")

# Gi·∫£m thi·ªÉu hi·ªáu ·ª©ng loading
st.markdown("""
<style>
    /* Gi·∫£m thi·ªÉu hi·ªáu ·ª©ng loading */
    .stApp > header {
        background-color: transparent;
    }
    
    /* Smooth transition cho sliders */
    .stSlider {
        transition: all 0.3s ease;
    }
    
    /* T·ªëi ∆∞u rendering */
    .main .block-container {
        padding-top: 1rem;
        padding-bottom: 1rem;
    }
    
    /* Loading spinner styling */
    .stSpinner {
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)

# ƒê·∫∑t t√™n file c·ªë ƒë·ªãnh (c√πng c·∫•p app.py)
LABEL_ENCODER_PATH = "label_encoder.pkl"
ONEHOT_ENCODER_PATH = "onehot_encoder.pkl"
TFIDF_VECTORIZER_PATH = "tfidf_vectorizer.pkl"
XGB_MODEL_PATH = "xgb_model.pkl"
COMPANY_FILE = "Overview_Companies.xlsx"
REVIEW_FILE = "Reviews.xlsx"
OVERVIEW_REVIEW_FILE = "Overview_Reviews.xlsx"

# Alternative function for sparse matrix concatenation when scipy is not available
def safe_hstack(matrices):
    """Safe horizontal stack that works with or without scipy"""
    if SCIPY_AVAILABLE:
        from scipy.sparse import hstack
        return hstack(matrices)
    else:
        # Fallback: convert to dense and use numpy
        dense_matrices = []
        for matrix in matrices:
            if hasattr(matrix, 'toarray'):
                dense_matrices.append(matrix.toarray())
            else:
                dense_matrices.append(np.array(matrix))
        return np.hstack(dense_matrices)

# Alternative cosine similarity function when sklearn is not available
def safe_cosine_similarity(X, Y=None):
    """Safe cosine similarity that works with or without sklearn"""
    if SKLEARN_AVAILABLE:
        from sklearn.metrics.pairwise import cosine_similarity
        return cosine_similarity(X, Y)
    else:
        # Simple numpy implementation
        if Y is None:
            Y = X
        
        # Normalize vectors
        X_norm = X / np.linalg.norm(X, axis=1, keepdims=True)
        Y_norm = Y / np.linalg.norm(Y, axis=1, keepdims=True)
        
        # Compute cosine similarity
        return np.dot(X_norm, Y_norm.T)

# ================== TEXT PREPROCESSING FUNCTIONS ==================
def clean_tokens(tokens):
    cleaned = [re.sub(r'\d+', '', word) for word in tokens]
    return [word.lower() for word in cleaned if word not in ['', ' ', ',', '.', '-', ':', '?', '%', '(', ')', '+', '/', 'g', 'ml']]

stop_words = set([
    "a", "an", "the", "in", "on", "at", "to", "from", "by", "of", "with", "and", "but", "or", "for", "nor", "so", "yet",
    "i", "you", "he", "she", "it", "we", "they", "me", "him", "her", "us", "them", "be", "have", "do", "does", "did",
    "was", "were", "will", "would", "shall", "should", "may", "might", "can", "could", "must",
    "that", "this", "which", "what", "their", "these", "those", "https", "www"
])

def remove_stopwords(tokens):
    return [word for word in tokens if word not in stop_words]

# ================== LOAD MODEL & ENCODER ==================
@st.cache_resource
def load_all_models():
    if not JOBLIB_AVAILABLE:
        st.warning("‚ö†Ô∏è joblib not available. Model loading disabled.")
        return None, None, None, None
        
    # Check if all model files exist
    model_files = [
        (LABEL_ENCODER_PATH, "Label Encoder"),
        (ONEHOT_ENCODER_PATH, "OneHot Encoder"), 
        (TFIDF_VECTORIZER_PATH, "TF-IDF Vectorizer"),
        (XGB_MODEL_PATH, "XGBoost Model")
    ]
    
    missing_files = []
    for file_path, file_name in model_files:
        if not os.path.exists(file_path):
            missing_files.append(f"{file_name} ({file_path})")
    
    if missing_files:
        st.warning(f"‚ö†Ô∏è Missing model files: {', '.join(missing_files)}")
        return None, None, None, None
    
    try:
        # Suppress scikit-learn version warnings
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=UserWarning, message=".*Trying to unpickle estimator.*")
            
            label_encoder = joblib.load(LABEL_ENCODER_PATH)
            onehot_encoder = joblib.load(ONEHOT_ENCODER_PATH)
            tfidf_vectorizer = joblib.load(TFIDF_VECTORIZER_PATH)
        
        # Load XGBoost model with warning suppression
        if XGBOOST_AVAILABLE:
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore", category=UserWarning, message=".*If you are loading a serialized model.*")
                xgb_model = joblib.load(XGB_MODEL_PATH)
        else:
            xgb_model = None
            
        st.success("‚úÖ Available models loaded successfully!")
        return label_encoder, onehot_encoder, tfidf_vectorizer, xgb_model
        
    except Exception as e:
        st.error(f"‚ùå Error loading models: {str(e)}")
        return None, None, None, None

label_encoder, onehot_encoder, tfidf_vectorizer, xgb_model = load_all_models()

# ================== LOAD DATA ==================
@st.cache_data
def load_data():
    df_companies = pd.read_excel(COMPANY_FILE)
    df_reviews = pd.read_excel(REVIEW_FILE)
    df_overview_reviews = pd.read_excel(OVERVIEW_REVIEW_FILE)
    return df_companies, df_reviews, df_overview_reviews

@st.cache_data
def load_and_process_recommendation_data():
    df = pd.read_excel(COMPANY_FILE)
    df = df[['Company Name', 'Company overview']].dropna().copy()
    
    if GENSIM_AVAILABLE:
        df['tokens'] = df['Company overview'].apply(lambda x: gensim.utils.simple_preprocess(x))
    else:
        # Simple tokenization fallback
        df['tokens'] = df['Company overview'].apply(lambda x: x.lower().split())
    
    df['tokens_cleaned'] = df['tokens'].apply(clean_tokens)
    df['tokens_final'] = df['tokens_cleaned'].apply(remove_stopwords)
    df = df[df['tokens_final'].str.len() > 0].copy()
    df['joined_tokens'] = df['tokens_final'].apply(lambda tokens: ' '.join(tokens))
    return df

df_companies, df_reviews, df_overview_reviews = load_data()

# ================== TITLE & SIDEBAR ==================
st.title("Project 02 - Company Recommendation & Candidate Classification")
st.caption("Team: Nguyen Quynh Oanh Thao - Nguyen Le Minh Quang")

# Show status information
if SKLEARN_AVAILABLE and not GENSIM_AVAILABLE:
    st.info("‚ÑπÔ∏è Running in **Basic Mode** - Core ML features available, advanced text processing disabled due to missing gensim.")
elif not SKLEARN_AVAILABLE:
    st.error("‚ö†Ô∏è Running in **Limited Mode** - Please install scikit-learn for full functionality.")
else:
    st.success("‚úÖ Running in **Full Mode** - All features available!")

# Show installation info if dependencies are missing
missing_deps = []
optional_deps = []

if not SKLEARN_AVAILABLE:
    missing_deps.append("scikit-learn (REQUIRED)")
if not PLOTTING_AVAILABLE:
    missing_deps.append("matplotlib/seaborn (REQUIRED)")

if not GENSIM_AVAILABLE:
    optional_deps.append("gensim (advanced text processing)")
if not XGBOOST_AVAILABLE:
    optional_deps.append("xgboost (advanced ML models)")
if not SCIPY_AVAILABLE:
    optional_deps.append("scipy (sparse matrix operations)")

if missing_deps or optional_deps:
    with st.expander("üîß Installation Status & Help"):
        if missing_deps:
            st.error("**‚ùå Missing Critical Dependencies:**")
            for dep in missing_deps:
                st.write(f"- {dep}")
            st.write("**To install missing packages:**")
            st.code("pip install -r requirements_minimal.txt", language="bash")
        
        if optional_deps:
            st.info("**‚ÑπÔ∏è Optional Dependencies (for enhanced features):**")
            for dep in optional_deps:
                st.write(f"- {dep}")
            st.write("**To install optional packages:**")
            st.code("pip install -r requirements.txt", language="bash")
        
        st.write("**Alternative installation methods:**")
        st.code("pip install streamlit pandas numpy scikit-learn matplotlib seaborn openpyxl", language="bash")
        st.write("**For version compatibility:**")
        st.code("pip install scikit-learn>=1.0.0,<1.4.0", language="bash")
        st.info("üí° If you encounter compilation errors, try using conda: `conda install scikit-learn matplotlib seaborn gensim`")
        
        if len(missing_deps) == 0:
            st.success("‚úÖ All core dependencies are available! Optional features may be limited.")

# Tabs for Topic 1 and Topic 2
tab1, tab2 = st.tabs(["üîç Topic 1: Company Recommendation", "üß† Topic 2: Candidate Classification"])

# ================== TOPIC 1: COMPANY RECOMMENDATION ==================
with tab1:
    st.header("Topic 1: Content-Based Company Recommendation System")
    
    if not SKLEARN_AVAILABLE:
        st.error("‚ùå scikit-learn is required for this feature. Please install it with: pip install scikit-learn")
        st.stop()
    
    if not GENSIM_AVAILABLE:
        st.info("‚ÑπÔ∏è **Note**: Advanced text similarity (Gensim) is not available. The system will use TF-IDF cosine similarity as an alternative.")
    
    # Load and process data for recommendation
    df_rec = load_and_process_recommendation_data()
    
    if df_rec is not None and not df_rec.empty:
        # Create TF-IDF vectorizer and transform
        vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 3), sublinear_tf=True, stop_words='english', min_df=2, max_df=0.8, norm='l2')
        X = vectorizer.fit_transform(df_rec['joined_tokens'])

        # Create dummy labels using KMeans
        kmeans = KMeans(n_clusters=3, random_state=42)
        df_rec['label'] = kmeans.fit_predict(X)
        label_map = {0: 'Low', 1: 'Medium', 2: 'High'}
        df_rec['label'] = df_rec['label'].map(label_map)

        # Encode & split
        le = LabelEncoder()
        y = le.fit_transform(df_rec['label'])
        
        # Create cosine similarity features
        cosine_sim = safe_cosine_similarity(X.toarray() if hasattr(X, 'toarray') else X)
        ref_sim = cosine_sim[0].reshape(-1, 1)
        X_with_sim = safe_hstack([X, ref_sim])
        
        X_train, X_test, y_train, y_test = train_test_split(X_with_sim, y, test_size=0.5, random_state=42, stratify=y)

        # Define models
        models = {
            "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced'),
            "K-Nearest Neighbors": KNeighborsClassifier(n_neighbors=5),
            "Decision Tree": DecisionTreeClassifier(class_weight='balanced'),
            "Random Forest": RandomForestClassifier(n_estimators=200, class_weight='balanced'),
            "Support Vector Machine": SVC(probability=True, class_weight='balanced')
        }

        # Evaluate models (c√≥ th·ªÉ collapse ƒë·ªÉ ti·∫øt ki·ªám kh√¥ng gian)
        with st.expander("üìä Xem Model Performance Analysis"):
            results = {}
            for name, model in models.items():
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
                recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
                f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
                accuracy = accuracy_score(y_test, y_pred)
                results[name] = {
                    "Accuracy": accuracy,
                    "Precision": precision,
                    "Recall": recall,
                    "F1-score": f1
                }

            # Display performance table
            st.write("## üìä Model Performance Summary")
            st.dataframe(pd.DataFrame(results).T.sort_values(by="F1-score", ascending=False))

            # Confusion matrix visualization
            st.write("## üîç Confusion Matrices")
            fig, axes = plt.subplots(2, 3, figsize=(18, 10))
            axes = axes.flatten()

            for idx, (name, model) in enumerate(models.items()):
                y_pred = model.predict(X_test)
                cm = confusion_matrix(y_test, y_pred)
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])
                axes[idx].set_title(f"{name}")
                axes[idx].set_xlabel("Predicted")
                axes[idx].set_ylabel("Actual")

            # Remove extra axes
            for j in range(len(models), len(axes)):
                fig.delaxes(axes[j])

            st.pyplot(fig)

        # Setup Gensim similarity (if available)
        if GENSIM_AVAILABLE:
            dictionary = corpora.Dictionary(df_rec['tokens_final'])
            corpus = [dictionary.doc2bow(text) for text in df_rec['tokens_final']]
            tfidf_model = gensim_models.TfidfModel(corpus)
            corpus_tfidf = tfidf_model[corpus]
            index = similarities.SparseMatrixSimilarity(corpus_tfidf, num_features=len(dictionary))
        else:
            dictionary = None
            corpus = None
            tfidf_model = None
            corpus_tfidf = None
            index = None

        # Selected model info
        st.markdown("## ‚úÖ Company Recommendation System")
        st.write("S·ª≠ d·ª•ng **Random Forest + TF-IDF + Cosine Similarity** ƒë·ªÉ ƒë·ªÅ xu·∫•t c√¥ng ty ph√π h·ª£p v·ªõi preferences c·ªßa b·∫°n.")
        st.info("üèÜ **Random Forest** ƒë∆∞·ª£c ch·ªçn l√†m model ch√≠nh d·ª±a tr√™n performance t·ªët nh·∫•t: F1-score = 0.8649")
        
        # Train Random Forest model cho prediction
        rf_model = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42)
        rf_model.fit(X_train, y_train)
        
        st.write("---")
        
        # ================ COMPANY RECOMMENDATION WITH SLIDERS ================
        st.subheader("üéØ T√¨m ki·∫øm c√¥ng ty ph√π h·ª£p v·ªõi b·∫°n")
        
        # Initialize session state
        if 'preferences_changed' not in st.session_state:
            st.session_state.preferences_changed = False
        
        # T·∫°o 2 c·ªôt cho input
        pref_col1, pref_col2 = st.columns(2)
        
        with pref_col1:
            st.write("### üìù M√¥ t·∫£ c√¥ng ty mong mu·ªën:")
            input_text = st.text_area("M√¥ t·∫£ v·ªÅ lo·∫°i c√¥ng ty b·∫°n mu·ªën l√†m vi·ªác:",
                                    placeholder="V√≠ d·ª•: C√¥ng ty c√¥ng ngh·ªá, m√¥i tr∆∞·ªùng nƒÉng ƒë·ªông, c∆° h·ªôi ph√°t tri·ªÉn, l√†m v·ªÅ AI/ML...",
                                    height=120,
                                    key="input_text_area")
            
            # Company type preference
            company_types = df_companies['Company Type'].dropna().unique().tolist()
            preferred_types = st.multiselect("Lo·∫°i h√¨nh c√¥ng ty ∆∞a th√≠ch:", 
                                           company_types,
                                           default=company_types[:3] if len(company_types) >= 3 else company_types,
                                           key="preferred_types_select")
            
            # Company size preference
            company_sizes = df_companies['Company size'].dropna().unique().tolist()
            preferred_sizes = st.multiselect("Quy m√¥ c√¥ng ty ∆∞a th√≠ch:",
                                           company_sizes,
                                           default=company_sizes[:2] if len(company_sizes) >= 2 else company_sizes,
                                           key="preferred_sizes_select")
        
        with pref_col2:
            st.write("### ‚öôÔ∏è Preferences c·ªßa b·∫°n:")
            
            # S·ª≠ d·ª•ng on_change callback ƒë·ªÉ tr√°nh rerun li√™n t·ª•c
            def update_preferences():
                st.session_state.preferences_changed = True
            
            # Slider cho c√°c y·∫øu t·ªë quan tr·ªçng v·ªõi key v√† on_change
            work_life_importance = st.slider("M·ª©c ƒë·ªô quan tr·ªçng c·ªßa Work-Life Balance (1-5)",
                                           min_value=1, max_value=5, value=4, step=1,
                                           help="B·∫°n coi tr·ªçng s·ª± c√¢n b·∫±ng c√¥ng vi·ªác - cu·ªôc s·ªëng ƒë·∫øn m·ª©c n√†o?",
                                           key="work_life_slider",
                                           on_change=update_preferences)
            
            career_importance = st.slider("M·ª©c ƒë·ªô quan tr·ªçng c·ªßa Career Development (1-5)",
                                        min_value=1, max_value=5, value=4, step=1,
                                        help="B·∫°n coi tr·ªçng c∆° h·ªôi ph√°t tri·ªÉn s·ª± nghi·ªáp ƒë·∫øn m·ª©c n√†o?",
                                        key="career_slider",
                                        on_change=update_preferences)
            
            salary_importance = st.slider("M·ª©c ƒë·ªô quan tr·ªçng c·ªßa Salary & Benefits (1-5)",
                                        min_value=1, max_value=5, value=3, step=1,
                                        help="B·∫°n coi tr·ªçng m·ª©c l∆∞∆°ng v√† ph√∫c l·ª£i ƒë·∫øn m·ª©c n√†o?",
                                        key="salary_slider",
                                        on_change=update_preferences)
            
            company_culture_importance = st.slider("M·ª©c ƒë·ªô quan tr·ªçng c·ªßa Company Culture (1-5)",
                                                  min_value=1, max_value=5, value=4, step=1,
                                                  help="B·∫°n coi tr·ªçng vƒÉn h√≥a c√¥ng ty ƒë·∫øn m·ª©c n√†o?",
                                                  key="culture_slider",
                                                  on_change=update_preferences)
            
            min_overall_rating = st.slider("Rating t·ªëi thi·ªÉu c·ªßa c√¥ng ty",
                                         min_value=1.0, max_value=5.0, value=3.5, step=0.1,
                                         help="Ch·ªâ hi·ªÉn th·ªã c√¥ng ty c√≥ overall rating >= gi√° tr·ªã n√†y",
                                         key="rating_slider",
                                         on_change=update_preferences)
            
            # S·ªë l∆∞·ª£ng k·∫øt qu·∫£
            num_results = st.selectbox("S·ªë l∆∞·ª£ng c√¥ng ty g·ª£i √Ω:", [3, 5, 8, 10], index=1,
                                     key="num_results_select")
        
        # N√∫t ƒë·ªÉ t√¨m c√¥ng ty ph√π h·ª£p
        if st.button("üîç T√¨m c√¥ng ty ph√π h·ª£p (Random Forest)", type="primary"):
            if not input_text.strip():
                st.warning("Vui l√≤ng nh·∫≠p m√¥ t·∫£ v·ªÅ c√¥ng ty mong mu·ªën.")
            else:
                # Th√™m spinner ƒë·ªÉ hi·ªÉn th·ªã loading
                with st.spinner('üîÑ ƒêang ph√¢n t√≠ch v√† t√¨m ki·∫øm c√¥ng ty ph√π h·ª£p...'):
                    # Process input text
                    if GENSIM_AVAILABLE:
                        input_tokens = gensim.utils.simple_preprocess(input_text)
                        input_tokens_clean = remove_stopwords(clean_tokens(input_tokens))
                        input_bow = dictionary.doc2bow(input_tokens_clean)

                        # Calculate similarities using Gensim
                        sims = index[tfidf_model[input_bow]]
                        ranked = sorted(enumerate(sims), key=lambda x: -x[1])
                    else:
                        # Fallback: simple text processing
                        input_tokens = input_text.lower().split()
                        input_tokens_clean = remove_stopwords(clean_tokens(input_tokens))
                        
                        # Use TF-IDF similarity as fallback
                        input_tfidf = vectorizer.transform([' '.join(input_tokens_clean)])
                        text_matrix = vectorizer.transform(df_rec['joined_tokens'])
                        sims = safe_cosine_similarity(input_tfidf, text_matrix)[0]
                        ranked = sorted(enumerate(sims), key=lambda x: -x[1])

                    # Use Random Forest to predict company fit levels
                    input_tfidf = vectorizer.transform([' '.join(input_tokens_clean)])
                    
                    # Filter companies based on preferences
                    filtered_companies = []
                    
                    # Progress bar for processing
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    total_companies = len(ranked)
                    processed = 0
                    
                    for idx, similarity_score in ranked:
                        # Update progress
                        processed += 1
                        progress = processed / total_companies
                        progress_bar.progress(progress)
                        status_text.text(f'ƒêang x·ª≠ l√Ω c√¥ng ty {processed}/{total_companies}')
                        
                        # L·∫•y th√¥ng tin c√¥ng ty t·ª´ df_companies
                        company_info = df_companies[df_companies['Company Name'] == df_rec.iloc[idx]['Company Name']]
                        
                        if not company_info.empty:
                            company_info = company_info.iloc[0]
                            
                            # L·∫•y th√¥ng tin rating t·ª´ df_overview_reviews
                            rating_info = df_overview_reviews[df_overview_reviews['Company Name'] == company_info['Company Name']]
                            overall_rating = rating_info['Overall rating'].iloc[0] if not rating_info.empty else 3.0  # Default rating if not found
                            
                            # L·ªçc theo lo·∫°i h√¨nh c√¥ng ty
                            if len(preferred_types) > 0:
                                if company_info['Company Type'] not in preferred_types:
                                    continue
                            
                            # L·ªçc theo quy m√¥ c√¥ng ty
                            if len(preferred_sizes) > 0:
                                if company_info['Company size'] not in preferred_sizes:
                                    continue
                            
                            # L·ªçc theo rating t·ªëi thi·ªÉu
                            if overall_rating < min_overall_rating:
                                continue
                            
                            # T√≠nh ƒëi·ªÉm d·ª±a tr√™n ƒë·ªô t∆∞∆°ng th√≠ch v√† c√°c y·∫øu t·ªë quan tr·ªçng
                            score = similarity_score * work_life_importance + \
                                    similarity_score * career_importance + \
                                    similarity_score * salary_importance + \
                                    similarity_score * company_culture_importance
                            
                            # Chu·∫©n b·ªã features cho Random Forest prediction (ph·∫£i match v·ªõi training data)
                            company_text_vector = vectorizer.transform([df_rec.iloc[idx]['joined_tokens']])
                            

                            # T√≠nh cosine similarity v·ªõi input
                            cosine_sim_score = safe_cosine_similarity(input_tfidf.toarray() if hasattr(input_tfidf, 'toarray') else input_tfidf, 
                                                                    company_text_vector.toarray() if hasattr(company_text_vector, 'toarray') else company_text_vector)[0][0]
                            

                            # K·∫øt h·ª£p features nh∆∞ l√∫c training: [text_features, cosine_similarity]
                            rf_features = safe_hstack([company_text_vector, np.array([[cosine_sim_score]])])
                            

                            # D·ª± ƒëo√°n m·ª©c ƒë·ªô ph√π h·ª£p c·ªßa c√¥ng ty v·ªõi Random Forest
                            try:
                                rf_prediction = rf_model.predict(rf_features)[0]
                                
                                # Ch·ªâ ƒë·ªãnh label t∆∞∆°ng ·ª©ng v·ªõi m·ª©c ƒë·ªô ph√π h·ª£p
                                if rf_prediction == 0:
                                    fit_label = "Low"
                                elif rf_prediction == 1:
                                    fit_label = "Medium"
                                else:
                                    fit_label = "High"
                                
                                # T√≠nh recommendation score t·ªïng h·ª£p
                                preference_score = (work_life_importance + career_importance + 
                                                  salary_importance + company_culture_importance) / 4
                                
                                recommendation_score = (
                                    similarity_score * 0.3 +  # Text similarity
                                    (overall_rating / 5.0) * 0.2 +  # Company rating
                                    (preference_score / 5.0) * 0.2 +  # User preferences
                                    cosine_sim_score * 0.3  # Cosine similarity
                                )
                                
                                # Th√™m th√¥ng tin c√¥ng ty v√†o danh s√°ch k·∫øt qu·∫£
                                filtered_companies.append({
                                    "Company Name": company_info['Company Name'],
                                    "Company Type": company_info['Company Type'],
                                    "Company size": company_info['Company size'],
                                    "Overall rating": overall_rating,
                                    "Fit Label": fit_label,
                                    "Similarity Score": similarity_score,
                                    "Cosine Score": cosine_sim_score,
                                    "Recommendation Score": recommendation_score,
                                    "Score": score
                                })
                                
                            except Exception as rf_error:
                                # Fallback n·∫øu RF prediction fail
                                st.warning(f"RF prediction failed for {company_info['Company Name']}: {str(rf_error)}")
                                
                                # Simple fallback classification
                                if similarity_score >= 0.7:
                                    fit_label = "High"
                                elif similarity_score >= 0.4:
                                    fit_label = "Medium"
                                else:
                                    fit_label = "Low"
                                
                                filtered_companies.append({
                                    "Company Name": company_info['Company Name'],
                                    "Company Type": company_info['Company Type'],
                                    "Company size": company_info['Company size'],
                                    "Overall rating": overall_rating,
                                    "Fit Label": fit_label,
                                    "Similarity Score": similarity_score,
                                    "Cosine Score": 0.0,
                                    "Recommendation Score": similarity_score,
                                    "Score": score
                                })
                    
                    # Clear progress indicators
                    progress_bar.empty()
                    status_text.empty()
                    
                    # Chuy·ªÉn ƒë·ªïi danh s√°ch k·∫øt qu·∫£ th√†nh DataFrame
                    if filtered_companies:
                        results_df = pd.DataFrame(filtered_companies)
                        
                        # S·∫Øp x·∫øp theo recommendation score thay v√¨ score c≈©
                        top_results = results_df.sort_values(by=["Recommendation Score", "Similarity Score"], 
                                                           ascending=[False, False]).head(num_results)
                        
                        # Hi·ªÉn th·ªã k·∫øt qu·∫£ v·ªõi th√¥ng tin chi ti·∫øt h∆°n
                        st.write(f"### üèÜ Top {min(num_results, len(filtered_companies))} c√¥ng ty ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t:")
                        
                        # Display results v·ªõi format c·∫£i thi·ªán
                        for idx, row in top_results.iterrows():
                            # T·∫°o container cho m·ªói c√¥ng ty
                            with st.container():
                                st.markdown(f"#### {idx+1}. üè¢ {row['Company Name']}")
                                
                                # Metrics row
                                col1, col2, col3, col4 = st.columns(4)
                                with col1:
                                    st.metric("Fit Level", row['Fit Label'])
                                with col2:
                                    st.metric("Overall Rating", f"{row['Overall rating']:.1f}‚≠ê")
                                with col3:
                                    st.metric("Text Similarity", f"{row['Similarity Score']:.3f}")
                                with col4:
                                    st.metric("Rec. Score", f"{row['Recommendation Score']:.3f}")
                                
                                # Company details
                                st.write(f"**Lo·∫°i h√¨nh:** {row['Company Type']} | **Quy m√¥:** {row['Company size']}")
                                
                                # Hi·ªÉn th·ªã th√™m th√¥ng tin c√¥ng ty khi nh·∫•n expand
                                with st.expander(f"üìÑ Xem chi ti·∫øt v·ªÅ {row['Company Name']}"):
                                    # L·∫•y company overview t·ª´ df_rec
                                    company_overview = df_rec[df_rec['Company Name'] == row['Company Name']]
                                    if not company_overview.empty:
                                        overview_text = company_overview.iloc[0]['joined_tokens']
                                        st.write("**Company Overview:**")
                                        st.write(overview_text[:300] + "..." if len(overview_text) > 300 else overview_text)
                                    
                                    st.write(f"**Cosine Similarity Score:** {row['Cosine Score']:.3f}")
                                    st.write(f"**Raw Score:** {row['Score']:.2f}")
                                
                                st.markdown("---")
                        
                        # Summary statistics
                        st.write("### üìä Th·ªëng k√™ k·∫øt qu·∫£:")
                        summary_col1, summary_col2, summary_col3 = st.columns(3)
                        
                        with summary_col1:
                            avg_rating = top_results['Overall rating'].mean()
                            st.metric("Average Rating", f"{avg_rating:.2f}‚≠ê")
                        
                        with summary_col2:
                            avg_rec_score = top_results['Recommendation Score'].mean()
                            st.metric("Average Rec. Score", f"{avg_rec_score:.3f}")
                        
                        with summary_col3:
                            high_fit_count = (top_results['Fit Label'] == 'High').sum()
                            st.metric("High Fit Companies", f"{high_fit_count}/{len(top_results)}")
                        
                        # Fit level distribution
                        fit_distribution = top_results['Fit Label'].value_counts()
                        st.write("**Ph√¢n b·ªë Fit Levels:**")
                        for level, count in fit_distribution.items():
                            percentage = (count / len(top_results)) * 100
                            st.write(f"- **{level}**: {count} companies ({percentage:.1f}%)")
                            
                    else:
                        st.warning("‚ùå Kh√¥ng t√¨m th·∫•y c√¥ng ty n√†o ph√π h·ª£p v·ªõi ti√™u ch√≠ c·ªßa b·∫°n. H√£y th·ª≠ ƒëi·ªÅu ch·ªânh filters.")
    else:
        st.warning("Kh√¥ng th·ªÉ t·∫£i ho·∫∑c c√¥ng ty kh√¥ng c√≥ d·ªØ li·ªáu.")

# ================== TOPIC 2: COMPANY RECOMMENDATION PREDICTION ==================
with tab2:
    st.header("Topic 2: Employee Recommendation Prediction")
    st.markdown("""
    **D·ª± ƒëo√°n xem nh√¢n vi√™n c√≥ recommend c√¥ng ty hay kh√¥ng d·ª±a tr√™n ƒë√°nh gi√° t·ª´ ITViec**
    
    Ch·ªçn m·ªôt c√¥ng ty c·ª• th·ªÉ ƒë·ªÉ:
    - üìä Xem ph√¢n t√≠ch t·ªïng quan v·ªÅ c√°c ƒë√°nh gi√°
    - ü§ñ Hu·∫•n luy·ªán model d·ª± ƒëo√°n recommendation
    - üîÆ D·ª± ƒëo√°n recommendation t·ª´ ƒë√°nh gi√° m·ªõi
    """)
    
    # Load additional libraries for Topic 2
    try:
        from underthesea import word_tokenize
        UNDERTHESEA_AVAILABLE = True
    except ImportError:
        UNDERTHESEA_AVAILABLE = False
        st.info("‚ÑπÔ∏è underthesea not available. Using simple tokenization.")
        
    from collections import Counter
    
    # Try to import SMOTE, but continue without it if not available
    try:
        from imblearn.over_sampling import SMOTE
        SMOTE_AVAILABLE = True
    except ImportError:
        SMOTE_AVAILABLE = False
        st.info("‚ÑπÔ∏è SMOTE not available for balancing data. Install with: pip install imbalanced-learn")

    # Load data function for Topic 2
    @st.cache_data
    def load_review_data():
        try:
            reviews = pd.read_excel(REVIEW_FILE)
            overview_reviews = pd.read_excel(OVERVIEW_REVIEW_FILE)
            
            # Check if Reviews already has Company Name (which it likely does based on earlier output)
            if 'Company Name' in reviews.columns:
                # Direct merge using Company Name
                data = reviews.merge(overview_reviews[["Company Name", "Overall rating"]], on="Company Name", how="left")
                
                # Fill missing ratings with default value
                data['Overall rating'] = data['Overall rating'].fillna(3.0)
                
                st.info(f"üìé Loaded {len(data)} reviews from {len(data['Company Name'].unique())} companies")
                return data
            else:
                # Fallback to ID-based merge if Company Name not present
                overview_companies = pd.read_excel(COMPANY_FILE)
                
                # Rename columns for consistency
                overview_reviews = overview_reviews.rename(columns={"id": "company_id"})
                overview_companies = overview_companies.rename(columns={"id": "company_id"})

                # Merge data
                data = reviews.merge(overview_reviews[["company_id", "Overall rating"]], left_on="id", right_on="company_id", how="left")
                data = data.merge(overview_companies[["company_id", "Company Name", "Company Type", "Company size"]], on="company_id", how="left")

                # Fill missing ratings with default value
                data['Overall rating'] = data['Overall rating'].fillna(3.0)
                
                st.info(f"üìé Loaded {len(data)} reviews from {len(data['Company Name'].unique())} companies")
                return data
                
        except Exception as e:
            st.error(f"‚ùå Error loading review data: {str(e)}")
            return None

    # Load Vietnamese stopwords and wrong words
    @st.cache_data
    def load_text_processing_data():
        try:
            # Try to load from local files first
            stopwords = set()
            wrong_words = set()
            
            # If files exist locally, load them
            import os
            if os.path.exists("vietnamese-stopwords.txt"):
                with open("vietnamese-stopwords.txt", encoding="utf-8") as f:
                    stopwords = set(f.read().splitlines())
            
            if os.path.exists("wrong-word.txt"):
                with open("wrong-word.txt", encoding="utf-8") as f:
                    wrong_words = set(f.read().splitlines())
            
            # If no local files, use default Vietnamese stopwords
            if not stopwords:
                stopwords = set([
                    "v√†", "c·ªßa", "l√†", "c√≥", "trong", "ƒë∆∞·ª£c", "cho", "t·ª´", "v·ªõi", "v·ªÅ", "n√†y", "ƒë√≥", "m·ªôt", "c√°c", "nh·ªØng", "ng∆∞·ªùi", "t√¥i", "b·∫°n", "h·ªç", "ch√∫ng", "ta", "kh√¥ng", "r·∫•t", "nhi·ªÅu", "√≠t", "l·∫°i", "c≈©ng", "ƒë√£", "s·∫Ω", "ƒëang", "th√¨", "ƒë·ªÉ", "khi", "n·∫øu", "m√†", "n√™n", "ph·∫£i", "n√≥", "vi·ªác", "l√∫c", "hay", "ho·∫∑c", "nh∆∞ng", "tuy", "v·∫´n", "ch·ªâ", "nh∆∞", "theo", "sau", "tr∆∞·ªõc", "ngo√†i", "gi·ªØa", "d∆∞·ªõi", "tr√™n", "g·∫ßn", "xa", "b√™n", "c·∫°nh"
                ])
            
            return stopwords, wrong_words
        except Exception as e:
            st.warning(f"Using default stopwords due to error: {str(e)}")
            return set(["v√†", "c·ªßa", "l√†", "c√≥", "trong", "ƒë∆∞·ª£c", "cho", "t·ª´", "v·ªõi"]), set()

    # Text cleaning function
    def clean_text(text, stopwords, wrong_words):
        if pd.isnull(text):
            return ""
        text = str(text).lower()
        text = re.sub(r'[^a-zA-Z0-9√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµ\s]', ' ', text)
        text = re.sub(r'\d+', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        # Use underthesea if available, otherwise use simple split
        if UNDERTHESEA_AVAILABLE:
            try:
                from underthesea import word_tokenize
                text = word_tokenize(text, format="text")
            except:
                pass  # Fall back to simple split
            
        words = [w for w in text.split() if w not in stopwords and w not in wrong_words and len(w) > 2]
        return " ".join(words)

    # Suggestion classification function
    def suggest_improvement(text):
        if pd.isnull(text):
            return "Kh√¥ng c√≥ g√≥p √Ω ti√™u c·ª±c"
        text = str(text).lower()
        negative_keywords = [
            "kh√¥ng", "thi·∫øu", "ch∆∞a", "overtime", "l∆∞∆°ng th·∫•p", "l∆∞∆°ng_th·∫•p",
            "√°p l·ª±c", "√°p_l·ª±c", "ch·∫≠m", "t·ªá", "b·∫•t c√¥ng", "b·∫•t_c√¥ng", "qu√° t·∫£i", "qu√°_t·∫£i", "stress"
        ]
        for kw in negative_keywords:
            if kw in text:
                return "C·∫ßn c·∫£i thi·ªán: " + kw.replace("_", " ")
        return "Kh√¥ng c√≥ g√≥p √Ω ti√™u c·ª±c"

    # Load data
    df_reviews = load_review_data()
    
    if df_reviews is not None and not df_reviews.empty:
        # Check for column name variations and standardize
        column_mappings = {
            'What I liked': ['What I liked', 'What I liked about the job', 'Liked'],
            'Suggestions for improvement': ['Suggestions for improvement', 'Suggestions', 'Improvement suggestions'],
            'Recommend?': ['Recommend?', 'Recommend', 'Would you recommend?', 'Recommend working here to a friend']
        }
        
        # Map columns to standard names
        for standard_name, variations in column_mappings.items():
            for variation in variations:
                if variation in df_reviews.columns:
                    if variation != standard_name:
                        df_reviews = df_reviews.rename(columns={variation: standard_name})
                    break
        
        # Check if required columns exist after mapping
        required_columns = ['What I liked', 'Suggestions for improvement', 'Recommend?']
        missing_columns = [col for col in required_columns if col not in df_reviews.columns]
        
        if missing_columns:
            st.error(f"‚ùå Missing required columns: {missing_columns}")
            st.write("Available columns:", df_reviews.columns.tolist())
            st.write("Expected columns:", required_columns)
        else:
            # Load text processing data
            stopwords, wrong_words = load_text_processing_data()
            
            # Apply text cleaning
            with st.spinner("üîÑ Processing review data..."):
                df_reviews['What I liked_clean'] = df_reviews['What I liked'].apply(lambda x: clean_text(x, stopwords, wrong_words))
                df_reviews['Suggestions_clean'] = df_reviews['Suggestions for improvement'].apply(suggest_improvement)
                df_reviews['text_combined'] = df_reviews['What I liked_clean'] + ' ' + df_reviews['Suggestions_clean']
            
            # Company selection section
            st.subheader("üè¢ Ch·ªçn c√¥ng ty ƒë·ªÉ ph√¢n t√≠ch")
            
            # Get list of companies with reviews
            companies_with_reviews = df_reviews['Company Name'].dropna().unique().tolist()
            companies_with_reviews.sort()
            
            if len(companies_with_reviews) > 0:
                selected_company = st.selectbox(
                    "Ch·ªçn c√¥ng ty:", 
                    companies_with_reviews,
                    key="company_select"
                )
                
                if selected_company:
                    # Filter reviews for selected company
                    company_reviews = df_reviews[df_reviews['Company Name'] == selected_company].copy()
                    
                    # Display company overview
                    st.subheader(f"üìä T·ªïng quan v·ªÅ {selected_company}")
                    
                    col1, col2, col3, col4 = st.columns(4)
                    with col1:
                        st.metric("T·ªïng s·ªë reviews", len(company_reviews))
                    with col2:
                        recommend_count = (company_reviews['Recommend?'] == 'Yes').sum()
                        st.metric("Recommend: Yes", recommend_count)
                    with col3:
                        not_recommend_count = (company_reviews['Recommend?'] == 'No').sum()
                        st.metric("Recommend: No", not_recommend_count)
                    with col4:
                        if len(company_reviews) > 0:
                            recommend_rate = (recommend_count / len(company_reviews)) * 100
                            st.metric("T·ª∑ l·ªá Recommend", f"{recommend_rate:.1f}%")
                        else:
                            st.metric("T·ª∑ l·ªá Recommend", "0%")
                    
                    # Show recommendation distribution for selected company
                    if len(company_reviews) > 0:
                        st.subheader("üìà Ph√¢n b·ªë Recommendation")
                        recommend_dist = company_reviews['Recommend?'].value_counts()
                        
                        if len(recommend_dist) > 0:
                            fig_company, ax = plt.subplots(figsize=(8, 4))
                            recommend_dist.plot(kind='bar', ax=ax, color=['#FF6B6B', '#4ECDC4'])
                            ax.set_title(f'Recommendation Distribution - {selected_company}')
                            ax.set_xlabel('Recommendation')
                            ax.set_ylabel('Count')
                            plt.xticks(rotation=0)
                            st.pyplot(fig_company)
                    
                    # Show sample reviews
                    st.subheader("üìù M·ªôt s·ªë ƒë√°nh gi√° m·∫´u")
                    sample_reviews = company_reviews.head(5)
                    
                    for idx, row in sample_reviews.iterrows():
                        with st.expander(f"Review {idx+1} - Recommend: {row['Recommend?']}"):
                            st.write("**ƒêi·ªÅu th√≠ch:**")
                            st.write(row['What I liked'] if pd.notna(row['What I liked']) else "Kh√¥ng c√≥ th√¥ng tin")
                            st.write("**G·ª£i √Ω c·∫£i thi·ªán:**")
                            st.write(row['Suggestions for improvement'] if pd.notna(row['Suggestions for improvement']) else "Kh√¥ng c√≥ g·ª£i √Ω")
                    
                    # Word analysis section
                    st.subheader("üìà Ph√¢n t√≠ch t·ª´ kh√≥a trong ƒë√°nh gi√°")
                    
                    if len(company_reviews) > 0:
                        # Analyze positive reviews (Recommend = Yes)
                        positive_reviews = company_reviews[company_reviews['Recommend?'] == 'Yes']
                        negative_reviews = company_reviews[company_reviews['Recommend?'] == 'No']
                        
                        col1, col2 = st.columns(2)
                        
                        with col1:
                            st.write("**üü¢ T·ª´ kh√≥a trong reviews RECOMMEND:**")
                            if len(positive_reviews) > 0:
                                positive_text = ' '.join(positive_reviews['What I liked_clean'].fillna('').astype(str))
                                if positive_text.strip():
                                    positive_words = positive_text.split()
                                    positive_word_freq = Counter(positive_words)
                                    top_positive = positive_word_freq.most_common(10)
                                    
                                    for word, count in top_positive:
                                        if len(word) > 2:  # Skip short words
                                            st.write(f"- {word}: {count} l·∫ßn")
                                else:
                                    st.write("Kh√¥ng c√≥ d·ªØ li·ªáu")
                            else:
                                st.write("Kh√¥ng c√≥ reviews recommend")
                        
                        with col2:
                            st.write("**üî¥ T·ª´ kh√≥a trong reviews KH√îNG RECOMMEND:**")
                            if len(negative_reviews) > 0:
                                negative_text = ' '.join(negative_reviews['What I liked_clean'].fillna('').astype(str))
                                if negative_text.strip():
                                    negative_words = negative_text.split()
                                    negative_word_freq = Counter(negative_words)
                                    top_negative = negative_word_freq.most_common(10)
                                    
                                    for word, count in top_negative:
                                        if len(word) > 2:  # Skip short words
                                            st.write(f"- {word}: {count} l·∫ßn")
                                else:
                                    st.write("Kh√¥ng c√≥ d·ªØ li·ªáu")
                            else:
                                st.write("Kh√¥ng c√≥ reviews kh√¥ng recommend")
                    
                    # Machine Learning Prediction Section
                    st.subheader("ü§ñ D·ª± ƒëo√°n Recommendation cho c√¥ng ty")
                    
                    # Train model if enough data
                    if len(df_reviews) > 100:
                        try:
                            # Feature engineering
                            tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')
                            X_text = tfidf.fit_transform(df_reviews['text_combined'].fillna(''))
                            
                            # Add numerical features
                            numerical_features = []
                            if 'Overall rating' in df_reviews.columns:
                                numerical_features.append(df_reviews['Overall rating'].fillna(3.0).values.reshape(-1, 1))
                            
                            # Combine features
                            if numerical_features:
                                if SKLEARN_AVAILABLE:
                                    scaler = StandardScaler()
                                    numerical_scaled = scaler.fit_transform(np.hstack(numerical_features))
                                    X_combined = safe_hstack([X_text, numerical_scaled])
                                else:
                                    st.error("‚ùå scikit-learn required for numerical feature scaling")
                                    X_combined = X_text
                            else:
                                X_combined = X_text
                            
                            # Convert to CSR format for indexing
                            X_combined = X_combined.tocsr()
                            
                            # Prepare target variable
                            y = df_reviews['Recommend?'].map({'Yes': 1, 'No': 0})
                            y = y.dropna()
                            
                            # Handle sparse matrix indexing properly
                            valid_indices = y.index.tolist()
                            X_final = X_combined[valid_indices]
                            
                            if len(y) > 50 and len(y.unique()) > 1:
                                # Split data
                                X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.3, random_state=42, stratify=y)
                                
                                # Train XGBoost model
                                with st.spinner("üîÑ Training prediction model..."):
                                    if XGBOOST_AVAILABLE:
                                        model = xgb.XGBClassifier(random_state=42, eval_metric='logloss')
                                    else:
                                        st.warning("‚ö†Ô∏è XGBoost not available. Using Random Forest instead.")
                                        model = RandomForestClassifier(random_state=42, n_estimators=100)
                                    
                                    model.fit(X_train, y_train)
                                    
                                    # Evaluate model
                                    y_pred = model.predict(X_test)
                                    accuracy = accuracy_score(y_test, y_pred)
                                    f1 = f1_score(y_test, y_pred)
                                    
                                    model_name = "XGBoost" if XGBOOST_AVAILABLE else "Random Forest"
                                    st.success(f"‚úÖ {model_name} model trained successfully! Accuracy: {accuracy:.3f}, F1-Score: {f1:.3f}")
                                
                                # Prediction for selected company
                                st.subheader("üîÆ D·ª± ƒëo√°n cho c√¥ng ty ƒë∆∞·ª£c ch·ªçn")
                                
                                # Get company-specific features
                                company_text_features = company_reviews['text_combined'].fillna('')
                                
                                if len(company_text_features) > 0:
                                    # Transform company features
                                    company_X_text = tfidf.transform(company_text_features)
                                    
                                    if numerical_features:
                                        company_numerical = company_reviews['Overall rating'].fillna(3.0).values.reshape(-1, 1)
                                        company_numerical_scaled = scaler.transform(company_numerical)
                                        company_X_combined = safe_hstack([company_X_text, company_numerical_scaled])
                                    else:
                                        company_X_combined = company_X_text
                                    
                                    # Make predictions
                                    predictions = model.predict(company_X_combined)
                                    prediction_probs = model.predict_proba(company_X_combined)
                                    
                                    # Calculate statistics
                                    predicted_recommend = np.sum(predictions)
                                    total_predictions = len(predictions)
                                    predicted_recommend_rate = (predicted_recommend / total_predictions) * 100
                                    
                                    # Display results
                                    col1, col2, col3 = st.columns(3)
                                    with col1:
                                        st.metric("Predicted Recommend", predicted_recommend)
                                    with col2:
                                        st.metric("Total Predictions", total_predictions)
                                    with col3:
                                        st.metric("Predicted Recommend Rate", f"{predicted_recommend_rate:.1f}%")
                                    
                                    # Show prediction confidence
                                    avg_confidence = np.mean(np.max(prediction_probs, axis=1))
                                    st.metric("Average Prediction Confidence", f"{avg_confidence:.3f}")
                                    
                                    # Compare with actual
                                    actual_recommend = (company_reviews['Recommend?'] == 'Yes').sum()
                                    actual_rate = (actual_recommend / len(company_reviews)) * 100
                                    
                                    st.subheader("üìä So s√°nh D·ª± ƒëo√°n vs Th·ª±c t·∫ø")
                                    
                                    # Create comparison using columns instead of dataframe to avoid Arrow conversion issues
                                    col1, col2 = st.columns(2)
                                    
                                    with col1:
                                        st.write("**Th·ª±c t·∫ø (Actual):**")
                                        st.write(f"- Recommend Count: {actual_recommend}")
                                        st.write(f"- Recommend Rate: {actual_rate:.1f}%")
                                    
                                    with col2:
                                        st.write("**D·ª± ƒëo√°n (Predicted):**")
                                        st.write(f"- Recommend Count: {predicted_recommend}")
                                        st.write(f"- Recommend Rate: {predicted_recommend_rate:.1f}%")
                                    
                                    # Show difference
                                    diff_count = predicted_recommend - actual_recommend
                                    diff_rate = predicted_recommend_rate - actual_rate
                                    
                                    st.write("**S·ª± kh√°c bi·ªát:**")
                                    if diff_count > 0:
                                        st.write(f"- Model d·ª± ƒëo√°n **cao h∆°n** {diff_count} recommendations ({diff_rate:+.1f}%)")
                                    elif diff_count < 0:
                                        st.write(f"- Model d·ª± ƒëo√°n **th·∫•p h∆°n** {abs(diff_count)} recommendations ({diff_rate:+.1f}%)")
                                    else:
                                        st.write("- Model d·ª± ƒëo√°n **ch√≠nh x√°c** s·ªë l∆∞·ª£ng recommendations")
                                    
                                    # Accuracy indicator
                                    accuracy_percentage = (1 - abs(diff_rate) / 100) * 100 if actual_rate > 0 else 0
                                    if accuracy_percentage > 90:
                                        st.success(f"üéØ ƒê·ªô ch√≠nh x√°c d·ª± ƒëo√°n: {accuracy_percentage:.1f}% (R·∫•t t·ªët)")
                                    elif accuracy_percentage > 70:
                                        st.info(f"üéØ ƒê·ªô ch√≠nh x√°c d·ª± ƒëo√°n: {accuracy_percentage:.1f}% (T·ªët)")
                                    else:
                                        st.warning(f"üéØ ƒê·ªô ch√≠nh x√°c d·ª± ƒëo√°n: {accuracy_percentage:.1f}% (C·∫ßn c·∫£i thi·ªán)")
                                    
                                    # Prediction distribution
                                    st.subheader("üéØ Ph√¢n b·ªë d·ª± ƒëo√°n")
                                    pred_dist = pd.Series(predictions).map({0: 'No', 1: 'Yes'}).value_counts()
                                    
                                    fig_pred, ax = plt.subplots(figsize=(8, 4))
                                    pred_dist.plot(kind='bar', ax=ax, color=['#FF6B6B', '#4ECDC4'])
                                    ax.set_title(f'Predicted Recommendation Distribution - {selected_company}')
                                    ax.set_xlabel('Prediction')
                                    ax.set_ylabel('Count')
                                    plt.xticks(rotation=0)
                                    st.pyplot(fig_pred)
                                    
                                    # Add user input section for custom prediction
                                    st.subheader("üí¨ D·ª± ƒëo√°n t·ª´ ƒë√°nh gi√° c·ªßa b·∫°n")
                                    st.write("Nh·∫≠p ƒë√°nh gi√° c·ªßa b·∫°n v·ªÅ c√¥ng ty ƒë·ªÉ xem model d·ª± ƒëo√°n b·∫°n c√≥ recommend hay kh√¥ng:")
                                    
                                    col1, col2 = st.columns(2)
                                    
                                    with col1:
                                        user_liked = st.text_area(
                                            "ƒêi·ªÅu b·∫°n th√≠ch v·ªÅ c√¥ng ty:",
                                            placeholder="V√≠ d·ª•: M√¥i tr∆∞·ªùng l√†m vi·ªác t·ªët, ƒë·ªìng nghi·ªáp h√≤a ƒë·ªìng, l∆∞∆°ng th∆∞·ªüng h·ª£p l√Ω...",
                                            height=100,
                                            key="user_liked_input"
                                        )
                                    
                                    with col2:
                                        user_suggestions = st.text_area(
                                            "G·ª£i √Ω c·∫£i thi·ªán:",
                                            placeholder="V√≠ d·ª•: C·∫ßn c·∫£i thi·ªán ch·∫ø ƒë·ªô l√†m vi·ªác, tƒÉng c∆° h·ªôi ƒë√†o t·∫°o...",
                                            height=100,
                                            key="user_suggestions_input"
                                        )
                                    
                                    # Company rating input
                                    user_rating = st.slider(
                                        "ƒê√°nh gi√° overall rating cho c√¥ng ty (1-5):",
                                        min_value=1.0, max_value=5.0, value=3.5, step=0.1,
                                        key="user_rating_input"
                                    )
                                    
                                    if st.button("üîÆ D·ª± ƒëo√°n t·ª´ ƒë√°nh gi√° c·ªßa t√¥i", type="primary"):
                                        if user_liked.strip() or user_suggestions.strip():
                                            # Process user input
                                            user_liked_clean = clean_text(user_liked, stopwords, wrong_words)
                                            user_suggestions_clean = suggest_improvement(user_suggestions)
                                            user_text_combined = user_liked_clean + ' ' + user_suggestions_clean
                                            
                                            # Transform user input
                                            user_X_text = tfidf.transform([user_text_combined])
                                            
                                            if numerical_features:
                                                user_numerical = np.array([[user_rating]])
                                                user_numerical_scaled = scaler.transform(user_numerical)
                                                user_X_combined = safe_hstack([user_X_text, user_numerical_scaled])
                                            else:
                                                user_X_combined = user_X_text
                                            
                                            # Make prediction
                                            user_prediction = model.predict(user_X_combined)[0]
                                            user_probability = model.predict_proba(user_X_combined)[0]
                                            
                                            # Display results
                                            st.subheader("üéØ K·∫øt qu·∫£ d·ª± ƒëo√°n")
                                            
                                            col1, col2, col3 = st.columns(3)
                                            
                                            with col1:
                                                if user_prediction == 1:
                                                    st.success("‚úÖ **B·∫†N S·∫º RECOMMEND**")
                                                else:
                                                    st.error("‚ùå **B·∫†N S·∫º KH√îNG RECOMMEND**")
                                            
                                            with col2:
                                                confidence = max(user_probability)
                                                st.metric("ƒê·ªô tin c·∫≠y", f"{confidence:.3f}")
                                            
                                            with col3:
                                                recommend_prob = user_probability[1]
                                                st.metric("X√°c su·∫•t Recommend", f"{recommend_prob:.3f}")
                                            
                                            # Detailed explanation
                                            st.write("**Ph√¢n t√≠ch chi ti·∫øt:**")
                                            st.write(f"- X√°c su·∫•t **Recommend**: {user_probability[1]:.3f}")
                                            st.write(f"- X√°c su·∫•t **Kh√¥ng Recommend**: {user_probability[0]:.3f}")
                                            st.write(f"- Overall Rating b·∫°n cho: {user_rating}/5.0")
                                            
                                            # Recommendation advice
                                            if user_prediction == 1:
                                                st.info("üí° **G·ª£i √Ω:** D·ª±a tr√™n ƒë√°nh gi√° c·ªßa b·∫°n, b·∫°n c√≥ xu h∆∞·ªõng recommend c√¥ng ty n√†y cho b·∫°n b√®.")
                                            else:
                                                st.warning("üí° **G·ª£i √Ω:** D·ª±a tr√™n ƒë√°nh gi√° c·ªßa b·∫°n, b·∫°n c√≥ th·ªÉ kh√¥ng recommend c√¥ng ty n√†y.")
                                            
                                            # Compare with company average
                                            if recommend_prob > predicted_recommend_rate / 100:
                                                st.success("üìä ƒê√°nh gi√° c·ªßa b·∫°n **t√≠ch c·ª±c h∆°n** trung b√¨nh c·ªßa c√¥ng ty")
                                            else:
                                                st.warning("üìä ƒê√°nh gi√° c·ªßa b·∫°n **ti√™u c·ª±c h∆°n** trung b√¨nh c·ªßa c√¥ng ty")
                                        else:
                                            st.warning("Vui l√≤ng nh·∫≠p √≠t nh·∫•t m·ªôt trong hai tr∆∞·ªùng: ƒëi·ªÅu th√≠ch ho·∫∑c g·ª£i √Ω c·∫£i thi·ªán.")
                                
                                else:
                                    st.warning("No text data available for prediction")
                                    
                            else:
                                st.warning("Not enough data for model training (need >50 samples with both Yes/No recommendations)")
                                
                        except Exception as e:
                            st.error(f"Error in model training: {str(e)}")
                    else:
                        st.warning("Not enough data for model training (need >100 reviews)")
                        
                        # Show basic statistics instead
                        st.subheader("üìà Th·ªëng k√™ c∆° b·∫£n")
                        if len(company_reviews) > 0:
                            actual_recommend = (company_reviews['Recommend?'] == 'Yes').sum()
                            actual_rate = (actual_recommend / len(company_reviews)) * 100
                            
                            st.write(f"**T·ª∑ l·ªá Recommend th·ª±c t·∫ø:** {actual_rate:.1f}%")
                            st.write(f"**D·ª±a tr√™n {len(company_reviews)} reviews hi·ªán c√≥**")
                            
                            if actual_rate >= 70:
                                st.success("üü¢ C√¥ng ty n√†y c√≥ t·ª∑ l·ªá recommendation cao!")
                            elif actual_rate >= 50:
                                st.info("üü° C√¥ng ty n√†y c√≥ t·ª∑ l·ªá recommendation trung b√¨nh")
                            else:
                                st.warning("üî¥ C√¥ng ty n√†y c√≥ t·ª∑ l·ªá recommendation th·∫•p")
            else:
                st.warning("No companies found in the review data")
    else:
        st.warning("‚ùå Could not load review data for classification.")

# ================== FOOTER ==================
st.write("---")
st.markdown("### ü§ñ V·ªÅ ch√∫ng t√¥i")
st.write("ƒê√¢y l√† h·ªá th·ªëng g·ª£i √Ω c√¥ng ty v√† ph√¢n lo·∫°i ·ª©ng vi√™n d·ª±a tr√™n n·ªôi dung CV v√† y√™u c·∫ßu c√¥ng vi·ªác.")
st.write("M·ªçi √Ω ki·∫øn ƒë√≥ng g√≥p xin g·ª≠i v·ªÅ email: contact@ourcompany.com")
st.markdown("### üìä C√¥ng ngh·ªá s·ª≠ d·ª•ng")
st.write("- Streamlit: Giao di·ªán ng∆∞·ªùi d√πng")
st.write("- Pandas, NumPy: X·ª≠ l√Ω d·ªØ li·ªáu")
st.write("- Scikit-learn: C√°c thu·∫≠t to√°n m√°y h·ªçc")
st.write("- Gensim: X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n")
st.write("- Matplotlib, Seaborn: V·∫Ω bi·ªÉu ƒë·ªì")
st.write("- XGBoost: M√¥ h√¨nh d·ª± ƒëo√°n n√¢ng cao")
st.write("- PyPDF2: X·ª≠ l√Ω file PDF")
st.write("- Joblib: L∆∞u tr·ªØ v√† t·∫£i m√¥ h√¨nh")
st.write("- OpenAI GPT-3.5: T·∫°o ph·∫£n h·ªìi v√† g·ª£i √Ω c·∫£i thi·ªán CV")
st.write("- Google Search API: T√¨m ki·∫øm th√¥ng tin c√¥ng ty")
st.write("- Email API: G·ª≠i email ch·ª©a CV")
st.write("- v√† nhi·ªÅu th∆∞ vi·ªán kh√°c...")
st.write("Ch√∫ng t√¥i li√™n t·ª•c c·∫£i thi·ªán h·ªá th·ªëng. Phi√™n b·∫£n hi·ªán t·∫°i: 1.0.0")
