import streamlit as st
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import gensim
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics.pairwise import cosine_similarity
from gensim import models as gensim_models, corpora, similarities
from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from scipy.sparse import hstack
import xgboost as xgb

from sklearn.metrics import (
    precision_score, recall_score, f1_score, accuracy_score, confusion_matrix,
    roc_auc_score, roc_curve
)

# PAGE CONFIG (n√™n ƒë·ªÉ ƒë·∫ßu file)
st.set_page_config(page_title="Project 02 - Company Recommendation & Candidate Classification", layout="wide")

# Gi·∫£m thi·ªÉu hi·ªáu ·ª©ng loading
st.markdown("""
<style>
    /* Gi·∫£m thi·ªÉu hi·ªáu ·ª©ng loading */
    .stApp > header {
        background-color: transparent;
    }
    
    /* Smooth transition cho sliders */
    .stSlider {
        transition: all 0.3s ease;
    }
    
    /* T·ªëi ∆∞u rendering */
    .main .block-container {
        padding-top: 1rem;
        padding-bottom: 1rem;
    }
    
    /* Loading spinner styling */
    .stSpinner {
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)

# ƒê·∫∑t t√™n file c·ªë ƒë·ªãnh (c√πng c·∫•p app.py)
LABEL_ENCODER_PATH = "label_encoder.pkl"
ONEHOT_ENCODER_PATH = "onehot_encoder.pkl"
TFIDF_VECTORIZER_PATH = "tfidf_vectorizer.pkl"
XGB_MODEL_PATH = "xgb_model.pkl"
COMPANY_FILE = "Overview_Companies.xlsx"
REVIEW_FILE = "Reviews.xlsx"
OVERVIEW_REVIEW_FILE = "Overview_Reviews.xlsx"

# ================== TEXT PREPROCESSING FUNCTIONS ==================
def clean_tokens(tokens):
    cleaned = [re.sub(r'\d+', '', word) for word in tokens]
    return [word.lower() for word in cleaned if word not in ['', ' ', ',', '.', '-', ':', '?', '%', '(', ')', '+', '/', 'g', 'ml']]

stop_words = set([
    "a", "an", "the", "in", "on", "at", "to", "from", "by", "of", "with", "and", "but", "or", "for", "nor", "so", "yet",
    "i", "you", "he", "she", "it", "we", "they", "me", "him", "her", "us", "them", "be", "have", "do", "does", "did",
    "was", "were", "will", "would", "shall", "should", "may", "might", "can", "could", "must",
    "that", "this", "which", "what", "their", "these", "those", "https", "www"
])

def remove_stopwords(tokens):
    return [word for word in tokens if word not in stop_words]

# ================== LOAD MODEL & ENCODER ==================
@st.cache_resource
def load_all_models():
    try:
        label_encoder = joblib.load(LABEL_ENCODER_PATH)
        onehot_encoder = joblib.load(ONEHOT_ENCODER_PATH)
        tfidf_vectorizer = joblib.load(TFIDF_VECTORIZER_PATH)
        xgb_model = joblib.load(XGB_MODEL_PATH)
        return label_encoder, onehot_encoder, tfidf_vectorizer, xgb_model
    except:
        return None, None, None, None

label_encoder, onehot_encoder, tfidf_vectorizer, xgb_model = load_all_models()

# ================== LOAD DATA ==================
@st.cache_data
def load_data():
    df_companies = pd.read_excel(COMPANY_FILE)
    df_reviews = pd.read_excel(REVIEW_FILE)
    df_overview_reviews = pd.read_excel(OVERVIEW_REVIEW_FILE)
    return df_companies, df_reviews, df_overview_reviews

@st.cache_data
def load_and_process_recommendation_data():
    df = pd.read_excel(COMPANY_FILE)
    df = df[['Company Name', 'Company overview']].dropna().copy()
    df['tokens'] = df['Company overview'].apply(lambda x: gensim.utils.simple_preprocess(x))
    df['tokens_cleaned'] = df['tokens'].apply(clean_tokens)
    df['tokens_final'] = df['tokens_cleaned'].apply(remove_stopwords)
    df = df[df['tokens_final'].str.len() > 0].copy()
    df['joined_tokens'] = df['tokens_final'].apply(lambda tokens: ' '.join(tokens))
    return df

df_companies, df_reviews, df_overview_reviews = load_data()

# ================== TITLE & SIDEBAR ==================
st.title("Project 02 - Company Recommendation & Candidate Classification")
st.caption("Team: Nguyen Quynh Oanh Thao - Nguyen Le Minh Quang")

# Tabs for Topic 1 and Topic 2
tab1, tab2 = st.tabs(["üîç Topic 1: Company Recommendation", "üß† Topic 2: Candidate Classification"])

# ================== TOPIC 1: COMPANY RECOMMENDATION ==================
with tab1:
    st.header("Topic 1: Content-Based Company Recommendation System")
    
    # Load and process data for recommendation
    df_rec = load_and_process_recommendation_data()
    
    if df_rec is not None and not df_rec.empty:
        # Create TF-IDF vectorizer and transform
        vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 3), sublinear_tf=True, stop_words='english', min_df=2, max_df=0.8, norm='l2')
        X = vectorizer.fit_transform(df_rec['joined_tokens'])

        # Create dummy labels using KMeans
        kmeans = KMeans(n_clusters=3, random_state=42)
        df_rec['label'] = kmeans.fit_predict(X)
        label_map = {0: 'Low', 1: 'Medium', 2: 'High'}
        df_rec['label'] = df_rec['label'].map(label_map)

        # Encode & split
        le = LabelEncoder()
        y = le.fit_transform(df_rec['label'])
        
        # Create cosine similarity features
        cosine_sim = cosine_similarity(X, X)
        ref_sim = cosine_sim[0].reshape(-1, 1)
        X_with_sim = hstack([X, ref_sim])
        
        X_train, X_test, y_train, y_test = train_test_split(X_with_sim, y, test_size=0.5, random_state=42, stratify=y)

        # Define models
        models = {
            "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced'),
            "K-Nearest Neighbors": KNeighborsClassifier(n_neighbors=5),
            "Decision Tree": DecisionTreeClassifier(class_weight='balanced'),
            "Random Forest": RandomForestClassifier(n_estimators=200, class_weight='balanced'),
            "Support Vector Machine": SVC(probability=True, class_weight='balanced')
        }

        # Evaluate models (c√≥ th·ªÉ collapse ƒë·ªÉ ti·∫øt ki·ªám kh√¥ng gian)
        with st.expander("üìä Xem Model Performance Analysis"):
            results = {}
            for name, model in models.items():
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
                recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
                f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
                accuracy = accuracy_score(y_test, y_pred)
                results[name] = {
                    "Accuracy": accuracy,
                    "Precision": precision,
                    "Recall": recall,
                    "F1-score": f1
                }

            # Display performance table
            st.write("## üìä Model Performance Summary")
            st.dataframe(pd.DataFrame(results).T.sort_values(by="F1-score", ascending=False))

            # Confusion matrix visualization
            st.write("## üîç Confusion Matrices")
            fig, axes = plt.subplots(2, 3, figsize=(18, 10))
            axes = axes.flatten()

            for idx, (name, model) in enumerate(models.items()):
                y_pred = model.predict(X_test)
                cm = confusion_matrix(y_test, y_pred)
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])
                axes[idx].set_title(f"{name}")
                axes[idx].set_xlabel("Predicted")
                axes[idx].set_ylabel("Actual")

            # Remove extra axes
            for j in range(len(models), len(axes)):
                fig.delaxes(axes[j])

            st.pyplot(fig)

        # Setup Gensim similarity
        dictionary = corpora.Dictionary(df_rec['tokens_final'])
        corpus = [dictionary.doc2bow(text) for text in df_rec['tokens_final']]
        tfidf_model = gensim_models.TfidfModel(corpus)
        corpus_tfidf = tfidf_model[corpus]
        index = similarities.SparseMatrixSimilarity(corpus_tfidf, num_features=len(dictionary))

        # Selected model info
        st.markdown("## ‚úÖ Company Recommendation System")
        st.write("S·ª≠ d·ª•ng **Random Forest + TF-IDF + Cosine Similarity** ƒë·ªÉ ƒë·ªÅ xu·∫•t c√¥ng ty ph√π h·ª£p v·ªõi preferences c·ªßa b·∫°n.")
        st.info("üèÜ **Random Forest** ƒë∆∞·ª£c ch·ªçn l√†m model ch√≠nh d·ª±a tr√™n performance t·ªët nh·∫•t: F1-score = 0.8649")
        
        # Train Random Forest model cho prediction
        rf_model = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42)
        rf_model.fit(X_train, y_train)
        
        st.write("---")
        
        # ================ COMPANY RECOMMENDATION WITH SLIDERS ================
        st.subheader("üéØ T√¨m ki·∫øm c√¥ng ty ph√π h·ª£p v·ªõi b·∫°n")
        
        # Initialize session state
        if 'preferences_changed' not in st.session_state:
            st.session_state.preferences_changed = False
        
        # T·∫°o 2 c·ªôt cho input
        pref_col1, pref_col2 = st.columns(2)
        
        with pref_col1:
            st.write("### üìù M√¥ t·∫£ c√¥ng ty mong mu·ªën:")
            input_text = st.text_area("M√¥ t·∫£ v·ªÅ lo·∫°i c√¥ng ty b·∫°n mu·ªën l√†m vi·ªác:",
                                    placeholder="V√≠ d·ª•: C√¥ng ty c√¥ng ngh·ªá, m√¥i tr∆∞·ªùng nƒÉng ƒë·ªông, c∆° h·ªôi ph√°t tri·ªÉn, l√†m v·ªÅ AI/ML...",
                                    height=120,
                                    key="input_text_area")
            
            # Company type preference
            company_types = df_companies['Company Type'].dropna().unique().tolist()
            preferred_types = st.multiselect("Lo·∫°i h√¨nh c√¥ng ty ∆∞a th√≠ch:", 
                                           company_types,
                                           default=company_types[:3] if len(company_types) >= 3 else company_types,
                                           key="preferred_types_select")
            
            # Company size preference
            company_sizes = df_companies['Company size'].dropna().unique().tolist()
            preferred_sizes = st.multiselect("Quy m√¥ c√¥ng ty ∆∞a th√≠ch:",
                                           company_sizes,
                                           default=company_sizes[:2] if len(company_sizes) >= 2 else company_sizes,
                                           key="preferred_sizes_select")
        
        with pref_col2:
            st.write("### ‚öôÔ∏è Preferences c·ªßa b·∫°n:")
            
            # S·ª≠ d·ª•ng on_change callback ƒë·ªÉ tr√°nh rerun li√™n t·ª•c
            def update_preferences():
                st.session_state.preferences_changed = True
            
            # Slider cho c√°c y·∫øu t·ªë quan tr·ªçng v·ªõi key v√† on_change
            work_life_importance = st.slider("M·ª©c ƒë·ªô quan tr·ªçng c·ªßa Work-Life Balance (1-5)",
                                           min_value=1, max_value=5, value=4, step=1,
                                           help="B·∫°n coi tr·ªçng s·ª± c√¢n b·∫±ng c√¥ng vi·ªác - cu·ªôc s·ªëng ƒë·∫øn m·ª©c n√†o?",
                                           key="work_life_slider",
                                           on_change=update_preferences)
            
            career_importance = st.slider("M·ª©c ƒë·ªô quan tr·ªçng c·ªßa Career Development (1-5)",
                                        min_value=1, max_value=5, value=4, step=1,
                                        help="B·∫°n coi tr·ªçng c∆° h·ªôi ph√°t tri·ªÉn s·ª± nghi·ªáp ƒë·∫øn m·ª©c n√†o?",
                                        key="career_slider",
                                        on_change=update_preferences)
            
            salary_importance = st.slider("M·ª©c ƒë·ªô quan tr·ªçng c·ªßa Salary & Benefits (1-5)",
                                        min_value=1, max_value=5, value=3, step=1,
                                        help="B·∫°n coi tr·ªçng m·ª©c l∆∞∆°ng v√† ph√∫c l·ª£i ƒë·∫øn m·ª©c n√†o?",
                                        key="salary_slider",
                                        on_change=update_preferences)
            
            company_culture_importance = st.slider("M·ª©c ƒë·ªô quan tr·ªçng c·ªßa Company Culture (1-5)",
                                                  min_value=1, max_value=5, value=4, step=1,
                                                  help="B·∫°n coi tr·ªçng vƒÉn h√≥a c√¥ng ty ƒë·∫øn m·ª©c n√†o?",
                                                  key="culture_slider",
                                                  on_change=update_preferences)
            
            min_overall_rating = st.slider("Rating t·ªëi thi·ªÉu c·ªßa c√¥ng ty",
                                         min_value=1.0, max_value=5.0, value=3.5, step=0.1,
                                         help="Ch·ªâ hi·ªÉn th·ªã c√¥ng ty c√≥ overall rating >= gi√° tr·ªã n√†y",
                                         key="rating_slider",
                                         on_change=update_preferences)
            
            # S·ªë l∆∞·ª£ng k·∫øt qu·∫£
            num_results = st.selectbox("S·ªë l∆∞·ª£ng c√¥ng ty g·ª£i √Ω:", [3, 5, 8, 10], index=1,
                                     key="num_results_select")
        
        # N√∫t ƒë·ªÉ t√¨m c√¥ng ty ph√π h·ª£p
        if st.button("üîç T√¨m c√¥ng ty ph√π h·ª£p (Random Forest)", type="primary"):
            if not input_text.strip():
                st.warning("Vui l√≤ng nh·∫≠p m√¥ t·∫£ v·ªÅ c√¥ng ty mong mu·ªën.")
            else:
                # Th√™m spinner ƒë·ªÉ hi·ªÉn th·ªã loading
                with st.spinner('üîÑ ƒêang ph√¢n t√≠ch v√† t√¨m ki·∫øm c√¥ng ty ph√π h·ª£p...'):
                    # Process input text
                    input_tokens = gensim.utils.simple_preprocess(input_text)
                    input_tokens_clean = remove_stopwords(clean_tokens(input_tokens))
                    input_bow = dictionary.doc2bow(input_tokens_clean)

                    # Calculate similarities using Gensim
                    sims = index[tfidf_model[input_bow]]
                    ranked = sorted(enumerate(sims), key=lambda x: -x[1])

                    # Use Random Forest to predict company fit levels
                    input_tfidf = vectorizer.transform([' '.join(input_tokens_clean)])
                    
                    # Filter companies based on preferences
                    filtered_companies = []
                    
                    # Progress bar for processing
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    total_companies = len(ranked)
                    processed = 0
                    
                    for idx, similarity_score in ranked:
                        # Update progress
                        processed += 1
                        progress = processed / total_companies
                        progress_bar.progress(progress)
                        status_text.text(f'ƒêang x·ª≠ l√Ω c√¥ng ty {processed}/{total_companies}')
                        
                        # L·∫•y th√¥ng tin c√¥ng ty t·ª´ df_companies
                        company_info = df_companies[df_companies['Company Name'] == df_rec.iloc[idx]['Company Name']]
                        
                        if not company_info.empty:
                            company_info = company_info.iloc[0]
                            
                            # L·ªçc theo lo·∫°i h√¨nh c√¥ng ty
                            if len(preferred_types) > 0:
                                if company_info['Company Type'] not in preferred_types:
                                    continue
                            
                            # L·ªçc theo quy m√¥ c√¥ng ty
                            if len(preferred_sizes) > 0:
                                if company_info['Company size'] not in preferred_sizes:
                                    continue
                            
                            # L·ªçc theo rating t·ªëi thi·ªÉu
                            if company_info['Overall rating'] < min_overall_rating:
                                continue
                            
                            # T√≠nh ƒëi·ªÉm d·ª±a tr√™n ƒë·ªô t∆∞∆°ng th√≠ch v√† c√°c y·∫øu t·ªë quan tr·ªçng
                            score = similarity_score * work_life_importance + \
                                    similarity_score * career_importance + \
                                    similarity_score * salary_importance + \
                                    similarity_score * company_culture_importance
                            
                            # Chu·∫©n b·ªã features cho Random Forest prediction (ph·∫£i match v·ªõi training data)
                            company_text_vector = vectorizer.transform([df_rec.iloc[idx]['joined_tokens']])
                            

                            # T√≠nh cosine similarity v·ªõi input
                            cosine_sim_score = cosine_similarity(input_tfidf, company_text_vector)[0][0]
                            

                            # K·∫øt h·ª£p features nh∆∞ l√∫c training: [text_features, cosine_similarity]
                            rf_features = hstack([company_text_vector, np.array([[cosine_sim_score]])])
                            

                            # D·ª± ƒëo√°n m·ª©c ƒë·ªô ph√π h·ª£p c·ªßa c√¥ng ty v·ªõi Random Forest
                            try:
                                rf_prediction = rf_model.predict(rf_features)[0]
                                
                                # Ch·ªâ ƒë·ªãnh label t∆∞∆°ng ·ª©ng v·ªõi m·ª©c ƒë·ªô ph√π h·ª£p
                                if rf_prediction == 0:
                                    fit_label = "Low"
                                elif rf_prediction == 1:
                                    fit_label = "Medium"
                                else:
                                    fit_label = "High"
                                
                                # T√≠nh recommendation score t·ªïng h·ª£p
                                preference_score = (work_life_importance + career_importance + 
                                                  salary_importance + company_culture_importance) / 4
                                
                                recommendation_score = (
                                    similarity_score * 0.3 +  # Text similarity
                                    (company_info['Overall rating'] / 5.0) * 0.2 +  # Company rating
                                    (preference_score / 5.0) * 0.2 +  # User preferences
                                    cosine_sim_score * 0.3  # Cosine similarity
                                )
                                
                                # Th√™m th√¥ng tin c√¥ng ty v√†o danh s√°ch k·∫øt qu·∫£
                                filtered_companies.append({
                                    "Company Name": company_info['Company Name'],
                                    "Company Type": company_info['Company Type'],
                                    "Company size": company_info['Company size'],
                                    "Overall rating": company_info['Overall rating'],
                                    "Fit Label": fit_label,
                                    "Similarity Score": similarity_score,
                                    "Cosine Score": cosine_sim_score,
                                    "Recommendation Score": recommendation_score,
                                    "Score": score
                                })
                                
                            except Exception as rf_error:
                                # Fallback n·∫øu RF prediction fail
                                st.warning(f"RF prediction failed for {company_info['Company Name']}: {str(rf_error)}")
                                
                                # Simple fallback classification
                                if similarity_score >= 0.7:
                                    fit_label = "High"
                                elif similarity_score >= 0.4:
                                    fit_label = "Medium"
                                else:
                                    fit_label = "Low"
                                
                                filtered_companies.append({
                                    "Company Name": company_info['Company Name'],
                                    "Company Type": company_info['Company Type'],
                                    "Company size": company_info['Company size'],
                                    "Overall rating": company_info['Overall rating'],
                                    "Fit Label": fit_label,
                                    "Similarity Score": similarity_score,
                                    "Cosine Score": 0.0,
                                    "Recommendation Score": similarity_score,
                                    "Score": score
                                })
                    
                    # Clear progress indicators
                    progress_bar.empty()
                    status_text.empty()
                    
                    # Chuy·ªÉn ƒë·ªïi danh s√°ch k·∫øt qu·∫£ th√†nh DataFrame
                    if filtered_companies:
                        results_df = pd.DataFrame(filtered_companies)
                        
                        # S·∫Øp x·∫øp theo recommendation score thay v√¨ score c≈©
                        top_results = results_df.sort_values(by=["Recommendation Score", "Similarity Score"], 
                                                           ascending=[False, False]).head(num_results)
                        
                        # Hi·ªÉn th·ªã k·∫øt qu·∫£ v·ªõi th√¥ng tin chi ti·∫øt h∆°n
                        st.write(f"### üèÜ Top {min(num_results, len(filtered_companies))} c√¥ng ty ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t:")
                        
                        # Display results v·ªõi format c·∫£i thi·ªán
                        for idx, row in top_results.iterrows():
                            # T·∫°o container cho m·ªói c√¥ng ty
                            with st.container():
                                st.markdown(f"#### {idx+1}. üè¢ {row['Company Name']}")
                                
                                # Metrics row
                                col1, col2, col3, col4 = st.columns(4)
                                with col1:
                                    st.metric("Fit Level", row['Fit Label'])
                                with col2:
                                    st.metric("Overall Rating", f"{row['Overall rating']:.1f}‚≠ê")
                                with col3:
                                    st.metric("Text Similarity", f"{row['Similarity Score']:.3f}")
                                with col4:
                                    st.metric("Rec. Score", f"{row['Recommendation Score']:.3f}")
                                
                                # Company details
                                st.write(f"**Lo·∫°i h√¨nh:** {row['Company Type']} | **Quy m√¥:** {row['Company size']}")
                                
                                # Hi·ªÉn th·ªã th√™m th√¥ng tin c√¥ng ty khi nh·∫•n expand
                                with st.expander(f"üìÑ Xem chi ti·∫øt v·ªÅ {row['Company Name']}"):
                                    # L·∫•y company overview t·ª´ df_rec
                                    company_overview = df_rec[df_rec['Company Name'] == row['Company Name']]
                                    if not company_overview.empty:
                                        overview_text = company_overview.iloc[0]['joined_tokens']
                                        st.write("**Company Overview:**")
                                        st.write(overview_text[:300] + "..." if len(overview_text) > 300 else overview_text)
                                    
                                    st.write(f"**Cosine Similarity Score:** {row['Cosine Score']:.3f}")
                                    st.write(f"**Raw Score:** {row['Score']:.2f}")
                                
                                st.markdown("---")
                        
                        # Summary statistics
                        st.write("### üìä Th·ªëng k√™ k·∫øt qu·∫£:")
                        summary_col1, summary_col2, summary_col3 = st.columns(3)
                        
                        with summary_col1:
                            avg_rating = top_results['Overall rating'].mean()
                            st.metric("Average Rating", f"{avg_rating:.2f}‚≠ê")
                        
                        with summary_col2:
                            avg_rec_score = top_results['Recommendation Score'].mean()
                            st.metric("Average Rec. Score", f"{avg_rec_score:.3f}")
                        
                        with summary_col3:
                            high_fit_count = (top_results['Fit Label'] == 'High').sum()
                            st.metric("High Fit Companies", f"{high_fit_count}/{len(top_results)}")
                        
                        # Fit level distribution
                        fit_distribution = top_results['Fit Label'].value_counts()
                        st.write("**Ph√¢n b·ªë Fit Levels:**")
                        for level, count in fit_distribution.items():
                            percentage = (count / len(top_results)) * 100
                            st.write(f"- **{level}**: {count} companies ({percentage:.1f}%)")
                            
                    else:
                        st.warning("‚ùå Kh√¥ng t√¨m th·∫•y c√¥ng ty n√†o ph√π h·ª£p v·ªõi ti√™u ch√≠ c·ªßa b·∫°n. H√£y th·ª≠ ƒëi·ªÅu ch·ªânh filters.")
    else:
        st.warning("Kh√¥ng th·ªÉ t·∫£i ho·∫∑c c√¥ng ty kh√¥ng c√≥ d·ªØ li·ªáu.")

# ================== TOPIC 2: CANDIDATE CLASSIFICATION ==================
with tab2:
    st.header("Topic 2: Candidate Classification System")
    
    @st.cache_data
    def load_candidate_classification_data():
        df = pd.read_excel(COMPANY_FILE)
        df = df[['Company Name', 'Company overview']].dropna().copy()
        df['tokens'] = df['Company overview'].apply(lambda x: gensim.utils.simple_preprocess(x))
        df['tokens_cleaned'] = df['tokens'].apply(clean_tokens)
        df['tokens_final'] = df['tokens_cleaned'].apply(remove_stopwords)
        df = df[df['tokens_final'].str.len() > 0].copy()
        df['joined_tokens'] = df['tokens_final'].apply(lambda tokens: ' '.join(tokens))
        return df

    df_candidates = load_candidate_classification_data()

    if df_candidates is not None and not df_candidates.empty and tfidf_vectorizer is not None and xgb_model is not None:
        try:
            # Vectorize the text data
            X_candidates = tfidf_vectorizer.transform(df_candidates['joined_tokens'])

            # Predict with the XGBoost model (s·ª≠a l·∫°i c√°ch predict)
            df_candidates['predicted_label'] = xgb_model.predict(X_candidates)

            # Map the predicted labels to actual labels (s·ª≠a l·∫°i c√°ch s·ª≠ d·ª•ng label encoder)
            if label_encoder is not None:
                df_candidates['predicted_label'] = label_encoder.inverse_transform(df_candidates['predicted_label'].astype(int))
            else:
                # N·∫øu kh√¥ng c√≥ label encoder, map tr·ª±c ti·∫øp
                label_map = {0: 'Low', 1: 'Medium', 2: 'High'}
                df_candidates['predicted_label'] = df_candidates['predicted_label'].map(label_map)

            st.write("D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë d·ª± ƒëo√°n v·ªÅ ph√¢n lo·∫°i ·ª©ng vi√™n:")
            st.dataframe(df_candidates[['Company Name', 'predicted_label']].head(10))
            
        except Exception as e:
            st.error(f"‚ùå L·ªói khi predict: {str(e)}")
            st.write("C√≥ th·ªÉ model ho·∫∑c vectorizer kh√¥ng t∆∞∆°ng th√≠ch v·ªõi d·ªØ li·ªáu hi·ªán t·∫°i.")
    else:
        st.warning("‚ùå Kh√¥ng th·ªÉ t·∫£i model, vectorizer ho·∫∑c d·ªØ li·ªáu ·ª©ng vi√™n.")

    st.write("---")
    
    # ================ UPLOAD CV SECTION ================
    st.subheader("üì§ T·∫£i l√™n CV c·ªßa b·∫°n ƒë·ªÉ ph√¢n lo·∫°i")
    uploaded_file = st.file_uploader("Ch·ªçn file CV c·ªßa b·∫°n (ƒë·ªãnh d·∫°ng .txt ho·∫∑c .pdf)", type=["txt", "pdf"])
    
    if uploaded_file is not None:
        try:
            # Read the uploaded file
            if uploaded_file.name.endswith('.txt'):
                cv_text = uploaded_file.read().decode("utf-8")
            else:
                # Convert PDF to text
                import PyPDF2
                pdf_reader = PyPDF2.PdfReader(uploaded_file)
                cv_text = ""
                for page in pdf_reader.pages:
                    cv_text += page.extract_text()
            
            # Ki·ªÉm tra n·∫øu c√≥ text
            if not cv_text.strip():
                st.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ ƒë·ªçc text t·ª´ file. Vui l√≤ng ki·ªÉm tra l·∫°i file.")
            else:
                # Preprocess the CV text
                cv_tokens = gensim.utils.simple_preprocess(cv_text)
                cv_tokens_clean = remove_stopwords(clean_tokens(cv_tokens))
                cv_joined = ' '.join(cv_tokens_clean)
                
                # Ki·ªÉm tra n·∫øu c√≥ text sau preprocessing
                if not cv_joined.strip():
                    st.warning("‚ö†Ô∏è Kh√¥ng c√≥ text h·ª£p l·ªá sau khi x·ª≠ l√Ω. Vui l√≤ng ki·ªÉm tra n·ªôi dung file.")
                else:
                    # Vectorize the CV
                    cv_vectorized = tfidf_vectorizer.transform([cv_joined])
                    
                    # Predict with the XGBoost model (s·ª≠a l·∫°i)
                    cv_prediction = xgb_model.predict(cv_vectorized)
                    
                    # X·ª≠ l√Ω k·∫øt qu·∫£ prediction
                    if label_encoder is not None:
                        cv_predicted_label = label_encoder.inverse_transform(cv_prediction.astype(int))[0]
                    else:
                        # Fallback mapping
                        label_map = {0: 'Low', 1: 'Medium', 2: 'High'}
                        cv_predicted_label = label_map.get(int(cv_prediction[0]), 'Unknown')
                    
                    # Hi·ªÉn th·ªã k·∫øt qu·∫£ v·ªõi style ƒë·∫πp h∆°n
                    st.success(f"‚úÖ **CV c·ªßa b·∫°n ƒë√£ ƒë∆∞·ª£c ph√¢n lo·∫°i l√†: {cv_predicted_label}**")
                    
                    # Hi·ªÉn th·ªã confidence score n·∫øu c√≥
                    try:
                        cv_proba = xgb_model.predict_proba(cv_vectorized)
                        max_proba = cv_proba.max()
                        st.info(f"üéØ **ƒê·ªô tin c·∫≠y:** {max_proba:.2%}")
                    except:
                        pass
                    
                    # Provide feedback and improvement suggestions
                    st.write("---")
                    st.subheader("üõ†Ô∏è G·ª£i √Ω c·∫£i thi·ªán CV")
                    
                    # Enhanced feedback
                    feedback = {
                        "High": "üî• **Tuy·ªát v·ªùi!** CV c·ªßa b·∫°n c√≥ ch·∫•t l∆∞·ª£ng cao. H√£y ti·∫øp t·ª•c duy tr√¨ v√† c·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n.",
                        "Medium": "üëç **Kh√° t·ªët!** CV c√≥ th·ªÉ ƒë∆∞·ª£c c·∫£i thi·ªán th√™m:\n- Th√™m c√°c d·ª± √°n c·ª• th·ªÉ\n- N√™u r√µ th√†nh t√≠ch b·∫±ng s·ªë li·ªáu\n- C·∫≠p nh·∫≠t k·ªπ nƒÉng m·ªõi",
                        "Low": "‚ö†Ô∏è **C·∫ßn c·∫£i thi·ªán:** CV c·∫ßn ƒë∆∞·ª£c n√¢ng c·∫•p:\n- B·ªï sung kinh nghi·ªám c·ª• th·ªÉ\n- Th√™m k·ªπ nƒÉng chuy√™n m√¥n\n- C·∫£i thi·ªán c√°ch tr√¨nh b√†y\n- Th√™m ch·ª©ng ch·ªâ/kh√≥a h·ªçc li√™n quan",
                        "IT/Software": "üíª **K·ªπ thu·∫≠t:** Nh·∫•n m·∫°nh k·ªπ nƒÉng l·∫≠p tr√¨nh, framework, v√† d·ª± √°n ƒë√£ th·ª±c hi·ªán.",
                        "Marketing": "üì¢ **Marketing:** Th√™m s·ªë li·ªáu c·ª• th·ªÉ v·ªÅ campaign v√† ROI ƒë√£ ƒë·∫°t ƒë∆∞·ª£c.",
                        "Sales": "üíº **Kinh doanh:** L√†m n·ªïi b·∫≠t k·ªπ nƒÉng ƒë√†m ph√°n v√† target ƒë√£ ho√†n th√†nh.",
                        "HR": "üë• **Nh√¢n s·ª±:** Nh·∫•n m·∫°nh kinh nghi·ªám qu·∫£n l√Ω ng∆∞·ªùi v√† gi·∫£i quy·∫øt xung ƒë·ªôt.",
                        "Finance": "üí∞ **T√†i ch√≠nh:** C·∫ßn c√°c ch·ª©ng ch·ªâ CPA, CFA v√† kinh nghi·ªám ph√¢n t√≠ch t√†i ch√≠nh."
                    }
                    
                    if cv_predicted_label in feedback:
                        st.markdown(feedback[cv_predicted_label])
                    else:
                        st.info("üìã H√£y ch·∫Øc ch·∫Øn r·∫±ng CV n√™u b·∫≠t ƒë∆∞·ª£c k·ªπ nƒÉng v√† kinh nghi·ªám li√™n quan ƒë·∫øn v·ªã tr√≠ ·ª©ng tuy·ªÉn.")
                    
                    # Hi·ªÉn th·ªã preview m·ªôt ph·∫ßn CV ƒë√£ x·ª≠ l√Ω
                    with st.expander("üëÄ Xem preview text ƒë√£ x·ª≠ l√Ω"):
                        st.text(cv_joined[:500] + "..." if len(cv_joined) > 500 else cv_joined)
        
        except Exception as e:
            st.error(f"‚ùå L·ªói khi x·ª≠ l√Ω CV: {str(e)}")
            st.write("Vui l√≤ng th·ª≠ l·∫°i v·ªõi file kh√°c ho·∫∑c ki·ªÉm tra ƒë·ªãnh d·∫°ng file.")
    
    # ================== CV STATISTICS ==================
    if df_candidates is not None and not df_candidates.empty:
        with st.expander("üìä Th·ªëng k√™ ph√¢n lo·∫°i CV"):
            if 'predicted_label' in df_candidates.columns:
                label_counts = df_candidates['predicted_label'].value_counts()
                
                col1, col2 = st.columns(2)
                with col1:
                    st.write("**Ph√¢n b·ªë labels:**")
                    for label, count in label_counts.items():
                        percentage = (count / len(df_candidates)) * 100
                        st.write(f"- **{label}**: {count} ({percentage:.1f}%)")
                
                with col2:
                    # T·∫°o simple bar chart
                    fig, ax = plt.subplots(figsize=(8, 4))
                    label_counts.plot(kind='bar', ax=ax, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])
                    ax.set_title('Ph√¢n b·ªë Classification Labels')
                    ax.set_xlabel('Labels')
                    ax.set_ylabel('S·ªë l∆∞·ª£ng')
                    plt.xticks(rotation=45)
                    st.pyplot(fig)
    # ================== FOOTER ==================
    st.write("---")
    st.markdown("### ü§ñ V·ªÅ ch√∫ng t√¥i")
    st.write("ƒê√¢y l√† h·ªá th·ªëng g·ª£i √Ω c√¥ng ty v√† ph√¢n lo·∫°i ·ª©ng vi√™n d·ª±a tr√™n n·ªôi dung CV v√† y√™u c·∫ßu c√¥ng vi·ªác.")
    st.write("M·ªçi √Ω ki·∫øn ƒë√≥ng g√≥p xin g·ª≠i v·ªÅ email: contact@ourcompany.com")
    st.markdown("### üìä C√¥ng ngh·ªá s·ª≠ d·ª•ng")
    st.write("- Streamlit: Giao di·ªán ng∆∞·ªùi d√πng")
    st.write("- Pandas, NumPy: X·ª≠ l√Ω d·ªØ li·ªáu")
    st.write("- Scikit-learn: C√°c thu·∫≠t to√°n m√°y h·ªçc")
    st.write("- Gensim: X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n")
    st.write("- Matplotlib, Seaborn: V·∫Ω bi·ªÉu ƒë·ªì")
    st.write("- XGBoost: M√¥ h√¨nh d·ª± ƒëo√°n n√¢ng cao")
    st.write("- PyPDF2: X·ª≠ l√Ω file PDF")
    st.write("- Joblib: L∆∞u tr·ªØ v√† t·∫£i m√¥ h√¨nh")
    st.write("- OpenAI GPT-3.5: T·∫°o ph·∫£n h·ªìi v√† g·ª£i √Ω c·∫£i thi·ªán CV")
    st.write("- Google Search API: T√¨m ki·∫øm th√¥ng tin c√¥ng ty")
    st.write("- Email API: G·ª≠i email ch·ª©a CV")
    st.write("- v√† nhi·ªÅu th∆∞ vi·ªán kh√°c...")
    st.write("Ch√∫ng t√¥i li√™n t·ª•c c·∫£i thi·ªán h·ªá th·ªëng. Phi√™n b·∫£n hi·ªán t·∫°i: 1.0.0")
