import streamlit as st
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import gensim
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics.pairwise import cosine_similarity
from gensim import models as gensim_models, corpora, similarities
from sklearn.cluster import KMeans
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from scipy.sparse import hstack
import xgboost as xgb

from sklearn.metrics import (
    precision_score, recall_score, f1_score, accuracy_score, confusion_matrix,
    roc_auc_score, roc_curve
)

# PAGE CONFIG (n√™n ƒë·ªÉ ƒë·∫ßu file)
st.set_page_config(page_title="Project 02 - Company Recommendation & Candidate Classification", layout="wide")

# Gi·∫£m thi·ªÉu hi·ªáu ·ª©ng loading
st.markdown("""
<style>
    /* Gi·∫£m thi·ªÉu hi·ªáu ·ª©ng loading */
    .stApp > header {
        background-color: transparent;
    }
    
    /* Smooth transition cho sliders */
    .stSlider {
        transition: all 0.3s ease;
    }
    
    /* T·ªëi ∆∞u rendering */
    .main .block-container {
        padding-top: 1rem;
        padding-bottom: 1rem;
    }
    
    /* Loading spinner styling */
    .stSpinner {
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)

# ƒê·∫∑t t√™n file c·ªë ƒë·ªãnh (c√πng c·∫•p app.py)
LABEL_ENCODER_PATH = "label_encoder.pkl"
ONEHOT_ENCODER_PATH = "onehot_encoder.pkl"
TFIDF_VECTORIZER_PATH = "tfidf_vectorizer.pkl"
XGB_MODEL_PATH = "xgb_model.pkl"
COMPANY_FILE = "Overview_Companies.xlsx"
REVIEW_FILE = "Reviews.xlsx"
OVERVIEW_REVIEW_FILE = "Overview_Reviews.xlsx"

# ================== TEXT PREPROCESSING FUNCTIONS ==================
def clean_tokens(tokens):
    cleaned = [re.sub(r'\d+', '', word) for word in tokens]
    return [word.lower() for word in cleaned if word not in ['', ' ', ',', '.', '-', ':', '?', '%', '(', ')', '+', '/', 'g', 'ml']]

stop_words = set([
    "a", "an", "the", "in", "on", "at", "to", "from", "by", "of", "with", "and", "but", "or", "for", "nor", "so", "yet",
    "i", "you", "he", "she", "it", "we", "they", "me", "him", "her", "us", "them", "be", "have", "do", "does", "did",
    "was", "were", "will", "would", "shall", "should", "may", "might", "can", "could", "must",
    "that", "this", "which", "what", "their", "these", "those", "https", "www"
])

def remove_stopwords(tokens):
    return [word for word in tokens if word not in stop_words]

# ================== LOAD MODEL & ENCODER ==================
@st.cache_resource
def load_all_models():
    import os
    import warnings
    
    # Check if all model files exist
    model_files = [
        (LABEL_ENCODER_PATH, "Label Encoder"),
        (ONEHOT_ENCODER_PATH, "OneHot Encoder"), 
        (TFIDF_VECTORIZER_PATH, "TF-IDF Vectorizer"),
        (XGB_MODEL_PATH, "XGBoost Model")
    ]
    
    missing_files = []
    for file_path, file_name in model_files:
        if not os.path.exists(file_path):
            missing_files.append(f"{file_name} ({file_path})")
    
    if missing_files:
        st.warning(f"‚ö†Ô∏è Missing model files: {', '.join(missing_files)}")
        return None, None, None, None
    
    try:
        label_encoder = joblib.load(LABEL_ENCODER_PATH)
        onehot_encoder = joblib.load(ONEHOT_ENCODER_PATH)
        tfidf_vectorizer = joblib.load(TFIDF_VECTORIZER_PATH)
        
        # Load XGBoost model with warning suppression
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=UserWarning, message=".*If you are loading a serialized model.*")
            xgb_model = joblib.load(XGB_MODEL_PATH)
            
        st.success("‚úÖ All models loaded successfully!")
        return label_encoder, onehot_encoder, tfidf_vectorizer, xgb_model
        
    except Exception as e:
        st.error(f"‚ùå Error loading models: {str(e)}")
        return None, None, None, None

label_encoder, onehot_encoder, tfidf_vectorizer, xgb_model = load_all_models()

# ================== LOAD DATA ==================
@st.cache_data
def load_data():
    df_companies = pd.read_excel(COMPANY_FILE)
    df_reviews = pd.read_excel(REVIEW_FILE)
    df_overview_reviews = pd.read_excel(OVERVIEW_REVIEW_FILE)
    return df_companies, df_reviews, df_overview_reviews

@st.cache_data
def load_and_process_recommendation_data():
    df = pd.read_excel(COMPANY_FILE)
    df = df[['Company Name', 'Company overview']].dropna().copy()
    df['tokens'] = df['Company overview'].apply(lambda x: gensim.utils.simple_preprocess(x))
    df['tokens_cleaned'] = df['tokens'].apply(clean_tokens)
    df['tokens_final'] = df['tokens_cleaned'].apply(remove_stopwords)
    df = df[df['tokens_final'].str.len() > 0].copy()
    df['joined_tokens'] = df['tokens_final'].apply(lambda tokens: ' '.join(tokens))
    return df

df_companies, df_reviews, df_overview_reviews = load_data()

# ================== TITLE & SIDEBAR ==================
st.title("Project 02 - Company Recommendation & Candidate Classification")
st.caption("Team: Nguyen Quynh Oanh Thao - Nguyen Le Minh Quang")

# Tabs for Topic 1 and Topic 2
tab1, tab2 = st.tabs(["üîç Topic 1: Company Recommendation", "üß† Topic 2: Candidate Classification"])

# ================== TOPIC 1: COMPANY RECOMMENDATION ==================
with tab1:
    st.header("Topic 1: Content-Based Company Recommendation System")
    
    # Load and process data for recommendation
    df_rec = load_and_process_recommendation_data()
    
    if df_rec is not None and not df_rec.empty:
        # Create TF-IDF vectorizer and transform
        vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 3), sublinear_tf=True, stop_words='english', min_df=2, max_df=0.8, norm='l2')
        X = vectorizer.fit_transform(df_rec['joined_tokens'])

        # Create dummy labels using KMeans
        kmeans = KMeans(n_clusters=3, random_state=42)
        df_rec['label'] = kmeans.fit_predict(X)
        label_map = {0: 'Low', 1: 'Medium', 2: 'High'}
        df_rec['label'] = df_rec['label'].map(label_map)

        # Encode & split
        le = LabelEncoder()
        y = le.fit_transform(df_rec['label'])
        
        # Create cosine similarity features
        cosine_sim = cosine_similarity(X, X)
        ref_sim = cosine_sim[0].reshape(-1, 1)
        X_with_sim = hstack([X, ref_sim])
        
        X_train, X_test, y_train, y_test = train_test_split(X_with_sim, y, test_size=0.5, random_state=42, stratify=y)

        # Define models
        models = {
            "Logistic Regression": LogisticRegression(max_iter=1000, class_weight='balanced'),
            "K-Nearest Neighbors": KNeighborsClassifier(n_neighbors=5),
            "Decision Tree": DecisionTreeClassifier(class_weight='balanced'),
            "Random Forest": RandomForestClassifier(n_estimators=200, class_weight='balanced'),
            "Support Vector Machine": SVC(probability=True, class_weight='balanced')
        }

        # Evaluate models (c√≥ th·ªÉ collapse ƒë·ªÉ ti·∫øt ki·ªám kh√¥ng gian)
        with st.expander("üìä Xem Model Performance Analysis"):
            results = {}
            for name, model in models.items():
                model.fit(X_train, y_train)
                y_pred = model.predict(X_test)
                precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
                recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
                f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)
                accuracy = accuracy_score(y_test, y_pred)
                results[name] = {
                    "Accuracy": accuracy,
                    "Precision": precision,
                    "Recall": recall,
                    "F1-score": f1
                }

            # Display performance table
            st.write("## üìä Model Performance Summary")
            st.dataframe(pd.DataFrame(results).T.sort_values(by="F1-score", ascending=False))

            # Confusion matrix visualization
            st.write("## üîç Confusion Matrices")
            fig, axes = plt.subplots(2, 3, figsize=(18, 10))
            axes = axes.flatten()

            for idx, (name, model) in enumerate(models.items()):
                y_pred = model.predict(X_test)
                cm = confusion_matrix(y_test, y_pred)
                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx])
                axes[idx].set_title(f"{name}")
                axes[idx].set_xlabel("Predicted")
                axes[idx].set_ylabel("Actual")

            # Remove extra axes
            for j in range(len(models), len(axes)):
                fig.delaxes(axes[j])

            st.pyplot(fig)

        # Setup Gensim similarity
        dictionary = corpora.Dictionary(df_rec['tokens_final'])
        corpus = [dictionary.doc2bow(text) for text in df_rec['tokens_final']]
        tfidf_model = gensim_models.TfidfModel(corpus)
        corpus_tfidf = tfidf_model[corpus]
        index = similarities.SparseMatrixSimilarity(corpus_tfidf, num_features=len(dictionary))

        # Selected model info
        st.markdown("## ‚úÖ Company Recommendation System")
        st.write("S·ª≠ d·ª•ng **Random Forest + TF-IDF + Cosine Similarity** ƒë·ªÉ ƒë·ªÅ xu·∫•t c√¥ng ty ph√π h·ª£p v·ªõi preferences c·ªßa b·∫°n.")
        st.info("üèÜ **Random Forest** ƒë∆∞·ª£c ch·ªçn l√†m model ch√≠nh d·ª±a tr√™n performance t·ªët nh·∫•t: F1-score = 0.8649")
        
        # Train Random Forest model cho prediction
        rf_model = RandomForestClassifier(n_estimators=200, class_weight='balanced', random_state=42)
        rf_model.fit(X_train, y_train)
        
        st.write("---")
        
        # ================ COMPANY RECOMMENDATION WITH SLIDERS ================
        st.subheader("üéØ T√¨m ki·∫øm c√¥ng ty ph√π h·ª£p v·ªõi b·∫°n")
        
        # Initialize session state
        if 'preferences_changed' not in st.session_state:
            st.session_state.preferences_changed = False
        
        # T·∫°o 2 c·ªôt cho input
        pref_col1, pref_col2 = st.columns(2)
        
        with pref_col1:
            st.write("### üìù M√¥ t·∫£ c√¥ng ty mong mu·ªën:")
            input_text = st.text_area("M√¥ t·∫£ v·ªÅ lo·∫°i c√¥ng ty b·∫°n mu·ªën l√†m vi·ªác:",
                                    placeholder="V√≠ d·ª•: C√¥ng ty c√¥ng ngh·ªá, m√¥i tr∆∞·ªùng nƒÉng ƒë·ªông, c∆° h·ªôi ph√°t tri·ªÉn, l√†m v·ªÅ AI/ML...",
                                    height=120,
                                    key="input_text_area")
            
            # Company type preference
            company_types = df_companies['Company Type'].dropna().unique().tolist()
            preferred_types = st.multiselect("Lo·∫°i h√¨nh c√¥ng ty ∆∞a th√≠ch:", 
                                           company_types,
                                           default=company_types[:3] if len(company_types) >= 3 else company_types,
                                           key="preferred_types_select")
            
            # Company size preference
            company_sizes = df_companies['Company size'].dropna().unique().tolist()
            preferred_sizes = st.multiselect("Quy m√¥ c√¥ng ty ∆∞a th√≠ch:",
                                           company_sizes,
                                           default=company_sizes[:2] if len(company_sizes) >= 2 else company_sizes,
                                           key="preferred_sizes_select")
        
        with pref_col2:
            st.write("### ‚öôÔ∏è Preferences c·ªßa b·∫°n:")
            
            # S·ª≠ d·ª•ng on_change callback ƒë·ªÉ tr√°nh rerun li√™n t·ª•c
            def update_preferences():
                st.session_state.preferences_changed = True
            
            # Slider cho c√°c y·∫øu t·ªë quan tr·ªçng v·ªõi key v√† on_change
            work_life_importance = st.slider("M·ª©c ƒë·ªô quan tr·ªçng c·ªßa Work-Life Balance (1-5)",
                                           min_value=1, max_value=5, value=4, step=1,
                                           help="B·∫°n coi tr·ªçng s·ª± c√¢n b·∫±ng c√¥ng vi·ªác - cu·ªôc s·ªëng ƒë·∫øn m·ª©c n√†o?",
                                           key="work_life_slider",
                                           on_change=update_preferences)
            
            career_importance = st.slider("M·ª©c ƒë·ªô quan tr·ªçng c·ªßa Career Development (1-5)",
                                        min_value=1, max_value=5, value=4, step=1,
                                        help="B·∫°n coi tr·ªçng c∆° h·ªôi ph√°t tri·ªÉn s·ª± nghi·ªáp ƒë·∫øn m·ª©c n√†o?",
                                        key="career_slider",
                                        on_change=update_preferences)
            
            salary_importance = st.slider("M·ª©c ƒë·ªô quan tr·ªçng c·ªßa Salary & Benefits (1-5)",
                                        min_value=1, max_value=5, value=3, step=1,
                                        help="B·∫°n coi tr·ªçng m·ª©c l∆∞∆°ng v√† ph√∫c l·ª£i ƒë·∫øn m·ª©c n√†o?",
                                        key="salary_slider",
                                        on_change=update_preferences)
            
            company_culture_importance = st.slider("M·ª©c ƒë·ªô quan tr·ªçng c·ªßa Company Culture (1-5)",
                                                  min_value=1, max_value=5, value=4, step=1,
                                                  help="B·∫°n coi tr·ªçng vƒÉn h√≥a c√¥ng ty ƒë·∫øn m·ª©c n√†o?",
                                                  key="culture_slider",
                                                  on_change=update_preferences)
            
            min_overall_rating = st.slider("Rating t·ªëi thi·ªÉu c·ªßa c√¥ng ty",
                                         min_value=1.0, max_value=5.0, value=3.5, step=0.1,
                                         help="Ch·ªâ hi·ªÉn th·ªã c√¥ng ty c√≥ overall rating >= gi√° tr·ªã n√†y",
                                         key="rating_slider",
                                         on_change=update_preferences)
            
            # S·ªë l∆∞·ª£ng k·∫øt qu·∫£
            num_results = st.selectbox("S·ªë l∆∞·ª£ng c√¥ng ty g·ª£i √Ω:", [3, 5, 8, 10], index=1,
                                     key="num_results_select")
        
        # N√∫t ƒë·ªÉ t√¨m c√¥ng ty ph√π h·ª£p
        if st.button("üîç T√¨m c√¥ng ty ph√π h·ª£p (Random Forest)", type="primary"):
            if not input_text.strip():
                st.warning("Vui l√≤ng nh·∫≠p m√¥ t·∫£ v·ªÅ c√¥ng ty mong mu·ªën.")
            else:
                # Th√™m spinner ƒë·ªÉ hi·ªÉn th·ªã loading
                with st.spinner('üîÑ ƒêang ph√¢n t√≠ch v√† t√¨m ki·∫øm c√¥ng ty ph√π h·ª£p...'):
                    # Process input text
                    input_tokens = gensim.utils.simple_preprocess(input_text)
                    input_tokens_clean = remove_stopwords(clean_tokens(input_tokens))
                    input_bow = dictionary.doc2bow(input_tokens_clean)

                    # Calculate similarities using Gensim
                    sims = index[tfidf_model[input_bow]]
                    ranked = sorted(enumerate(sims), key=lambda x: -x[1])

                    # Use Random Forest to predict company fit levels
                    input_tfidf = vectorizer.transform([' '.join(input_tokens_clean)])
                    
                    # Filter companies based on preferences
                    filtered_companies = []
                    
                    # Progress bar for processing
                    progress_bar = st.progress(0)
                    status_text = st.empty()
                    
                    total_companies = len(ranked)
                    processed = 0
                    
                    for idx, similarity_score in ranked:
                        # Update progress
                        processed += 1
                        progress = processed / total_companies
                        progress_bar.progress(progress)
                        status_text.text(f'ƒêang x·ª≠ l√Ω c√¥ng ty {processed}/{total_companies}')
                        
                        # L·∫•y th√¥ng tin c√¥ng ty t·ª´ df_companies
                        company_info = df_companies[df_companies['Company Name'] == df_rec.iloc[idx]['Company Name']]
                        
                        if not company_info.empty:
                            company_info = company_info.iloc[0]
                            
                            # L·∫•y th√¥ng tin rating t·ª´ df_overview_reviews
                            rating_info = df_overview_reviews[df_overview_reviews['Company Name'] == company_info['Company Name']]
                            overall_rating = rating_info['Overall rating'].iloc[0] if not rating_info.empty else 3.0  # Default rating if not found
                            
                            # L·ªçc theo lo·∫°i h√¨nh c√¥ng ty
                            if len(preferred_types) > 0:
                                if company_info['Company Type'] not in preferred_types:
                                    continue
                            
                            # L·ªçc theo quy m√¥ c√¥ng ty
                            if len(preferred_sizes) > 0:
                                if company_info['Company size'] not in preferred_sizes:
                                    continue
                            
                            # L·ªçc theo rating t·ªëi thi·ªÉu
                            if overall_rating < min_overall_rating:
                                continue
                            
                            # T√≠nh ƒëi·ªÉm d·ª±a tr√™n ƒë·ªô t∆∞∆°ng th√≠ch v√† c√°c y·∫øu t·ªë quan tr·ªçng
                            score = similarity_score * work_life_importance + \
                                    similarity_score * career_importance + \
                                    similarity_score * salary_importance + \
                                    similarity_score * company_culture_importance
                            
                            # Chu·∫©n b·ªã features cho Random Forest prediction (ph·∫£i match v·ªõi training data)
                            company_text_vector = vectorizer.transform([df_rec.iloc[idx]['joined_tokens']])
                            

                            # T√≠nh cosine similarity v·ªõi input
                            cosine_sim_score = cosine_similarity(input_tfidf, company_text_vector)[0][0]
                            

                            # K·∫øt h·ª£p features nh∆∞ l√∫c training: [text_features, cosine_similarity]
                            rf_features = hstack([company_text_vector, np.array([[cosine_sim_score]])])
                            

                            # D·ª± ƒëo√°n m·ª©c ƒë·ªô ph√π h·ª£p c·ªßa c√¥ng ty v·ªõi Random Forest
                            try:
                                rf_prediction = rf_model.predict(rf_features)[0]
                                
                                # Ch·ªâ ƒë·ªãnh label t∆∞∆°ng ·ª©ng v·ªõi m·ª©c ƒë·ªô ph√π h·ª£p
                                if rf_prediction == 0:
                                    fit_label = "Low"
                                elif rf_prediction == 1:
                                    fit_label = "Medium"
                                else:
                                    fit_label = "High"
                                
                                # T√≠nh recommendation score t·ªïng h·ª£p
                                preference_score = (work_life_importance + career_importance + 
                                                  salary_importance + company_culture_importance) / 4
                                
                                recommendation_score = (
                                    similarity_score * 0.3 +  # Text similarity
                                    (overall_rating / 5.0) * 0.2 +  # Company rating
                                    (preference_score / 5.0) * 0.2 +  # User preferences
                                    cosine_sim_score * 0.3  # Cosine similarity
                                )
                                
                                # Th√™m th√¥ng tin c√¥ng ty v√†o danh s√°ch k·∫øt qu·∫£
                                filtered_companies.append({
                                    "Company Name": company_info['Company Name'],
                                    "Company Type": company_info['Company Type'],
                                    "Company size": company_info['Company size'],
                                    "Overall rating": overall_rating,
                                    "Fit Label": fit_label,
                                    "Similarity Score": similarity_score,
                                    "Cosine Score": cosine_sim_score,
                                    "Recommendation Score": recommendation_score,
                                    "Score": score
                                })
                                
                            except Exception as rf_error:
                                # Fallback n·∫øu RF prediction fail
                                st.warning(f"RF prediction failed for {company_info['Company Name']}: {str(rf_error)}")
                                
                                # Simple fallback classification
                                if similarity_score >= 0.7:
                                    fit_label = "High"
                                elif similarity_score >= 0.4:
                                    fit_label = "Medium"
                                else:
                                    fit_label = "Low"
                                
                                filtered_companies.append({
                                    "Company Name": company_info['Company Name'],
                                    "Company Type": company_info['Company Type'],
                                    "Company size": company_info['Company size'],
                                    "Overall rating": overall_rating,
                                    "Fit Label": fit_label,
                                    "Similarity Score": similarity_score,
                                    "Cosine Score": 0.0,
                                    "Recommendation Score": similarity_score,
                                    "Score": score
                                })
                    
                    # Clear progress indicators
                    progress_bar.empty()
                    status_text.empty()
                    
                    # Chuy·ªÉn ƒë·ªïi danh s√°ch k·∫øt qu·∫£ th√†nh DataFrame
                    if filtered_companies:
                        results_df = pd.DataFrame(filtered_companies)
                        
                        # S·∫Øp x·∫øp theo recommendation score thay v√¨ score c≈©
                        top_results = results_df.sort_values(by=["Recommendation Score", "Similarity Score"], 
                                                           ascending=[False, False]).head(num_results)
                        
                        # Hi·ªÉn th·ªã k·∫øt qu·∫£ v·ªõi th√¥ng tin chi ti·∫øt h∆°n
                        st.write(f"### üèÜ Top {min(num_results, len(filtered_companies))} c√¥ng ty ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t:")
                        
                        # Display results v·ªõi format c·∫£i thi·ªán
                        for idx, row in top_results.iterrows():
                            # T·∫°o container cho m·ªói c√¥ng ty
                            with st.container():
                                st.markdown(f"#### {idx+1}. üè¢ {row['Company Name']}")
                                
                                # Metrics row
                                col1, col2, col3, col4 = st.columns(4)
                                with col1:
                                    st.metric("Fit Level", row['Fit Label'])
                                with col2:
                                    st.metric("Overall Rating", f"{row['Overall rating']:.1f}‚≠ê")
                                with col3:
                                    st.metric("Text Similarity", f"{row['Similarity Score']:.3f}")
                                with col4:
                                    st.metric("Rec. Score", f"{row['Recommendation Score']:.3f}")
                                
                                # Company details
                                st.write(f"**Lo·∫°i h√¨nh:** {row['Company Type']} | **Quy m√¥:** {row['Company size']}")
                                
                                # Hi·ªÉn th·ªã th√™m th√¥ng tin c√¥ng ty khi nh·∫•n expand
                                with st.expander(f"üìÑ Xem chi ti·∫øt v·ªÅ {row['Company Name']}"):
                                    # L·∫•y company overview t·ª´ df_rec
                                    company_overview = df_rec[df_rec['Company Name'] == row['Company Name']]
                                    if not company_overview.empty:
                                        overview_text = company_overview.iloc[0]['joined_tokens']
                                        st.write("**Company Overview:**")
                                        st.write(overview_text[:300] + "..." if len(overview_text) > 300 else overview_text)
                                    
                                    st.write(f"**Cosine Similarity Score:** {row['Cosine Score']:.3f}")
                                    st.write(f"**Raw Score:** {row['Score']:.2f}")
                                
                                st.markdown("---")
                        
                        # Summary statistics
                        st.write("### üìä Th·ªëng k√™ k·∫øt qu·∫£:")
                        summary_col1, summary_col2, summary_col3 = st.columns(3)
                        
                        with summary_col1:
                            avg_rating = top_results['Overall rating'].mean()
                            st.metric("Average Rating", f"{avg_rating:.2f}‚≠ê")
                        
                        with summary_col2:
                            avg_rec_score = top_results['Recommendation Score'].mean()
                            st.metric("Average Rec. Score", f"{avg_rec_score:.3f}")
                        
                        with summary_col3:
                            high_fit_count = (top_results['Fit Label'] == 'High').sum()
                            st.metric("High Fit Companies", f"{high_fit_count}/{len(top_results)}")
                        
                        # Fit level distribution
                        fit_distribution = top_results['Fit Label'].value_counts()
                        st.write("**Ph√¢n b·ªë Fit Levels:**")
                        for level, count in fit_distribution.items():
                            percentage = (count / len(top_results)) * 100
                            st.write(f"- **{level}**: {count} companies ({percentage:.1f}%)")
                            
                    else:
                        st.warning("‚ùå Kh√¥ng t√¨m th·∫•y c√¥ng ty n√†o ph√π h·ª£p v·ªõi ti√™u ch√≠ c·ªßa b·∫°n. H√£y th·ª≠ ƒëi·ªÅu ch·ªânh filters.")
    else:
        st.warning("Kh√¥ng th·ªÉ t·∫£i ho·∫∑c c√¥ng ty kh√¥ng c√≥ d·ªØ li·ªáu.")

# ================== TOPIC 2: CANDIDATE FIT CLASSIFICATION ==================
with tab2:
    st.header("Topic 2: Candidate Fit Classification")
    st.markdown("D·ª±a tr√™n th√¥ng tin ƒë√°nh gi√° t·ª´ nh√¢n vi√™n ƒë√£ review tr√™n ITViec, d·ª± ƒëo√°n xem h·ªç c√≥ recommend c√¥ng ty hay kh√¥ng.")
    
    # Load additional libraries for Topic 2
    try:
        from underthesea import word_tokenize
        from collections import Counter
        from scipy.sparse import hstack
        from sklearn.preprocessing import StandardScaler
        
        # Try to import SMOTE, but continue without it if not available
        try:
            from imblearn.over_sampling import SMOTE
            SMOTE_AVAILABLE = True
        except ImportError:
            SMOTE_AVAILABLE = False
            st.info("‚ÑπÔ∏è SMOTE not available for balancing data. Install with: pip install imbalanced-learn")
            
    except ImportError as e:
        st.error(f"‚ùå Missing required libraries: {str(e)}")
        st.info("Please install: pip install underthesea imbalanced-learn xgboost")
        st.stop()

    # Load data function for Topic 2
    @st.cache_data
    def load_review_data():
        try:
            reviews = pd.read_excel(REVIEW_FILE)
            overview_reviews = pd.read_excel(OVERVIEW_REVIEW_FILE)
            
            # Check if Reviews already has Company Name (which it likely does based on earlier output)
            if 'Company Name' in reviews.columns:
                # Direct merge using Company Name
                data = reviews.merge(overview_reviews[["Company Name", "Overall rating"]], on="Company Name", how="left")
                
                # Fill missing ratings with default value
                data['Overall rating'] = data['Overall rating'].fillna(3.0)
                
                st.info(f"üìé Loaded {len(data)} reviews from {len(data['Company Name'].unique())} companies")
                return data
            else:
                # Fallback to ID-based merge if Company Name not present
                overview_companies = pd.read_excel(COMPANY_FILE)
                
                # Rename columns for consistency
                overview_reviews = overview_reviews.rename(columns={"id": "company_id"})
                overview_companies = overview_companies.rename(columns={"id": "company_id"})

                # Merge data
                data = reviews.merge(overview_reviews[["company_id", "Overall rating"]], left_on="id", right_on="company_id", how="left")
                data = data.merge(overview_companies[["company_id", "Company Name", "Company Type", "Company size"]], on="company_id", how="left")

                # Fill missing ratings with default value
                data['Overall rating'] = data['Overall rating'].fillna(3.0)
                
                st.info(f"üìé Loaded {len(data)} reviews from {len(data['Company Name'].unique())} companies")
                return data
                
        except Exception as e:
            st.error(f"‚ùå Error loading review data: {str(e)}")
            return None

    # Load Vietnamese stopwords and wrong words
    @st.cache_data
    def load_text_processing_data():
        try:
            # Try to load from local files first
            stopwords = set()
            wrong_words = set()
            
            # If files exist locally, load them
            import os
            if os.path.exists("vietnamese-stopwords.txt"):
                with open("vietnamese-stopwords.txt", encoding="utf-8") as f:
                    stopwords = set(f.read().splitlines())
            
            if os.path.exists("wrong-word.txt"):
                with open("wrong-word.txt", encoding="utf-8") as f:
                    wrong_words = set(f.read().splitlines())
            
            # If no local files, use default Vietnamese stopwords
            if not stopwords:
                stopwords = set([
                    "v√†", "c·ªßa", "l√†", "c√≥", "trong", "ƒë∆∞·ª£c", "cho", "t·ª´", "v·ªõi", "v·ªÅ", "n√†y", "ƒë√≥", "m·ªôt", "c√°c", "nh·ªØng", "ng∆∞·ªùi", "t√¥i", "b·∫°n", "h·ªç", "ch√∫ng", "ta", "kh√¥ng", "r·∫•t", "nhi·ªÅu", "√≠t", "l·∫°i", "c≈©ng", "ƒë√£", "s·∫Ω", "ƒëang", "th√¨", "ƒë·ªÉ", "khi", "n·∫øu", "m√†", "n√™n", "ph·∫£i", "n√≥", "vi·ªác", "l√∫c", "hay", "ho·∫∑c", "nh∆∞ng", "tuy", "v·∫´n", "ch·ªâ", "nh∆∞", "theo", "sau", "tr∆∞·ªõc", "ngo√†i", "gi·ªØa", "d∆∞·ªõi", "tr√™n", "g·∫ßn", "xa", "b√™n", "c·∫°nh"
                ])
            
            return stopwords, wrong_words
        except Exception as e:
            st.warning(f"Using default stopwords due to error: {str(e)}")
            return set(["v√†", "c·ªßa", "l√†", "c√≥", "trong", "ƒë∆∞·ª£c", "cho", "t·ª´", "v·ªõi"]), set()

    # Text cleaning function
    def clean_text(text, stopwords, wrong_words):
        if pd.isnull(text):
            return ""
        text = str(text).lower()
        text = re.sub(r'[^a-zA-Z0-9√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠ƒë√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≠√¨·ªâƒ©·ªã√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµ\s]', ' ', text)
        text = re.sub(r'\d+', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        
        # Use simple split if underthesea fails
        try:
            text = word_tokenize(text, format="text")
        except:
            pass
            
        words = [w for w in text.split() if w not in stopwords and w not in wrong_words and len(w) > 2]
        return " ".join(words)

    # Suggestion classification function
    def suggest_improvement(text):
        if pd.isnull(text):
            return "Kh√¥ng c√≥ g√≥p √Ω ti√™u c·ª±c"
        text = str(text).lower()
        negative_keywords = [
            "kh√¥ng", "thi·∫øu", "ch∆∞a", "overtime", "l∆∞∆°ng th·∫•p", "l∆∞∆°ng_th·∫•p",
            "√°p l·ª±c", "√°p_l·ª±c", "ch·∫≠m", "t·ªá", "b·∫•t c√¥ng", "b·∫•t_c√¥ng", "qu√° t·∫£i", "qu√°_t·∫£i", "stress"
        ]
        for kw in negative_keywords:
            if kw in text:
                return "C·∫ßn c·∫£i thi·ªán: " + kw.replace("_", " ")
        return "Kh√¥ng c√≥ g√≥p √Ω ti√™u c·ª±c"

    # Load data
    df_reviews = load_review_data()
    
    if df_reviews is not None and not df_reviews.empty:
        # Check for column name variations and standardize
        column_mappings = {
            'What I liked': ['What I liked', 'What I liked about the job', 'Liked'],
            'Suggestions for improvement': ['Suggestions for improvement', 'Suggestions', 'Improvement suggestions'],
            'Recommend?': ['Recommend?', 'Recommend', 'Would you recommend?', 'Recommend working here to a friend']
        }
        
        # Map columns to standard names
        for standard_name, variations in column_mappings.items():
            for variation in variations:
                if variation in df_reviews.columns:
                    if variation != standard_name:
                        df_reviews = df_reviews.rename(columns={variation: standard_name})
                    break
        
        # Check if required columns exist after mapping
        required_columns = ['What I liked', 'Suggestions for improvement', 'Recommend?']
        missing_columns = [col for col in required_columns if col not in df_reviews.columns]
        
        if missing_columns:
            st.error(f"‚ùå Missing required columns: {missing_columns}")
            st.write("Available columns:", df_reviews.columns.tolist())
            st.write("Expected columns:", required_columns)
        else:
            # Load text processing data
            stopwords, wrong_words = load_text_processing_data()
            
            # Apply text cleaning
            with st.spinner("üîÑ Processing review data..."):
                df_reviews['What I liked_clean'] = df_reviews['What I liked'].apply(lambda x: clean_text(x, stopwords, wrong_words))
                df_reviews['Suggestions_clean'] = df_reviews['Suggestions for improvement'].apply(suggest_improvement)
                df_reviews['text_combined'] = df_reviews['What I liked_clean'] + ' ' + df_reviews['Suggestions_clean']
            
            # Display data overview
            st.subheader("üìä Data Overview")
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Total Reviews", len(df_reviews))
            with col2:
                recommend_dist = df_reviews['Recommend?'].value_counts()
                st.metric("Recommend: Yes", recommend_dist.get('Yes', 0))
            with col3:
                st.metric("Recommend: No", recommend_dist.get('No', 0))
            
            # Show recommendation distribution
            if len(recommend_dist) > 0:
                st.subheader("üìà Recommendation Distribution")
                fig_dist, ax = plt.subplots(figsize=(8, 4))
                recommend_dist.plot(kind='bar', ax=ax, color=['#FF6B6B', '#4ECDC4'])
                ax.set_title('Distribution of Recommendations')
                ax.set_xlabel('Recommendation')
                ax.set_ylabel('Count')
                plt.xticks(rotation=0)
                st.pyplot(fig_dist)
                
            # Add model training and prediction section
            st.subheader("ü§ñ Machine Learning Models")
            
            # Prepare features for ML
            if len(df_reviews) > 100:  # Only train if we have enough data
                try:
                    # Feature engineering
                    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), stop_words='english')
                    X_text = tfidf.fit_transform(df_reviews['text_combined'].fillna(''))
                    
                    # Add numerical features
                    numerical_features = []
                    if 'Overall rating' in df_reviews.columns:
                        numerical_features.append(df_reviews['Overall rating'].fillna(3.0).values.reshape(-1, 1))
                    
                    # Combine features
                    if numerical_features:
                        scaler = StandardScaler()
                        numerical_scaled = scaler.fit_transform(np.hstack(numerical_features))
                        X_combined = hstack([X_text, numerical_scaled])
                    else:
                        X_combined = X_text
                    
                    # Convert to CSR format for indexing
                    X_combined = X_combined.tocsr()
                    
                    # Prepare target variable
                    y = df_reviews['Recommend?'].map({'Yes': 1, 'No': 0})
                    y = y.dropna()
                    
                    # Handle sparse matrix indexing properly
                    valid_indices = y.index.tolist()
                    X_final = X_combined[valid_indices]
                    
                    if len(y) > 50 and len(y.unique()) > 1:  # Ensure we have enough samples and both classes
                        # Split data
                        X_train, X_test, y_train, y_test = train_test_split(X_final, y, test_size=0.3, random_state=42, stratify=y)
                        
                        # Handle imbalanced data
                        if SMOTE_AVAILABLE and len(y_train) > 100:
                            try:
                                smote = SMOTE(random_state=42)
                                X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)
                                st.info("‚úÖ Data balanced using SMOTE")
                            except Exception as smote_error:
                                X_train_balanced, y_train_balanced = X_train, y_train
                                st.warning(f"SMOTE failed: {smote_error}. Using original data.")
                        else:
                            X_train_balanced, y_train_balanced = X_train, y_train
                            if not SMOTE_AVAILABLE:
                                st.info("‚ÑπÔ∏è Using original data without balancing")
                        
                        # Train models
                        models = {
                            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),
                            'SVM': SVC(random_state=42, probability=True),
                            'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='logloss')
                        }
                        
                        results = {}
                        for name, model in models.items():
                            with st.spinner(f"Training {name}..."):
                                model.fit(X_train_balanced, y_train_balanced)
                                y_pred = model.predict(X_test)
                                accuracy = accuracy_score(y_test, y_pred)
                                f1 = f1_score(y_test, y_pred)
                                results[name] = {'accuracy': accuracy, 'f1': f1, 'model': model}
                        
                        # Display results
                        st.write("### Model Performance:")
                        results_df = pd.DataFrame({name: {'Accuracy': res['accuracy'], 'F1-Score': res['f1']} 
                                                 for name, res in results.items()}).T
                        st.dataframe(results_df.round(4))
                        
                        # Show best model
                        best_model_name = max(results.keys(), key=lambda x: results[x]['f1'])
                        st.success(f"üèÜ Best Model: **{best_model_name}** (F1-Score: {results[best_model_name]['f1']:.4f})")
                        
                    else:
                        st.warning("Not enough data for model training (need >50 samples with both Yes/No recommendations)")
                        
                except Exception as e:
                    st.error(f"Error in model training: {str(e)}")
            else:
                st.warning("Not enough data for model training (need >100 reviews)")
    else:
        st.warning("‚ùå Could not load review data for classification.")

# ================== FOOTER ==================
st.write("---")
st.markdown("### ü§ñ V·ªÅ ch√∫ng t√¥i")
st.write("ƒê√¢y l√† h·ªá th·ªëng g·ª£i √Ω c√¥ng ty v√† ph√¢n lo·∫°i ·ª©ng vi√™n d·ª±a tr√™n n·ªôi dung CV v√† y√™u c·∫ßu c√¥ng vi·ªác.")
st.write("M·ªçi √Ω ki·∫øn ƒë√≥ng g√≥p xin g·ª≠i v·ªÅ email: contact@ourcompany.com")
st.markdown("### üìä C√¥ng ngh·ªá s·ª≠ d·ª•ng")
st.write("- Streamlit: Giao di·ªán ng∆∞·ªùi d√πng")
st.write("- Pandas, NumPy: X·ª≠ l√Ω d·ªØ li·ªáu")
st.write("- Scikit-learn: C√°c thu·∫≠t to√°n m√°y h·ªçc")
st.write("- Gensim: X·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n")
st.write("- Matplotlib, Seaborn: V·∫Ω bi·ªÉu ƒë·ªì")
st.write("- XGBoost: M√¥ h√¨nh d·ª± ƒëo√°n n√¢ng cao")
st.write("- PyPDF2: X·ª≠ l√Ω file PDF")
st.write("- Joblib: L∆∞u tr·ªØ v√† t·∫£i m√¥ h√¨nh")
st.write("- OpenAI GPT-3.5: T·∫°o ph·∫£n h·ªìi v√† g·ª£i √Ω c·∫£i thi·ªán CV")
st.write("- Google Search API: T√¨m ki·∫øm th√¥ng tin c√¥ng ty")
st.write("- Email API: G·ª≠i email ch·ª©a CV")
st.write("- v√† nhi·ªÅu th∆∞ vi·ªán kh√°c...")
st.write("Ch√∫ng t√¥i li√™n t·ª•c c·∫£i thi·ªán h·ªá th·ªëng. Phi√™n b·∫£n hi·ªán t·∫°i: 1.0.0")
